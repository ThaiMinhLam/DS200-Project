"""
Tham kháº£o: https://github.com/ds4v/absa-vlsp-2018/blob/main/processors/vietnamese_processor.py
"""

import re
import requests
from io import StringIO

class VietnameseNormalizer:
    """
    Tham kháº£o: https://github.com/VinAIResearch/BARTpho/blob/main/VietnameseToneNormalization.md
    """
    VINAI_NORMALIZED_TONE = {
        'Ã²a': 'oÃ ', 'Ã’a': 'OÃ ', 'Ã’A': 'OÃ€', 
        'Ã³a': 'oÃ¡', 'Ã“a': 'OÃ¡', 'Ã“A': 'OÃ', 
        'á»a': 'oáº£', 'á»a': 'Oáº£', 'á»A': 'Oáº¢',
        'Ãµa': 'oÃ£', 'Ã•a': 'OÃ£', 'Ã•A': 'OÃƒ',
        'á»a': 'oáº¡', 'á»Œa': 'Oáº¡', 'á»ŒA': 'Oáº ',
        'Ã²e': 'oÃ¨', 'Ã’e': 'OÃ¨', 'Ã’E': 'OÃˆ',
        'Ã³e': 'oÃ©', 'Ã“e': 'OÃ©', 'Ã“E': 'OÃ‰',
        'á»e': 'oáº»', 'á»e': 'Oáº»', 'á»E': 'Oáºº',
        'Ãµe': 'oáº½', 'Ã•e': 'Oáº½', 'Ã•E': 'Oáº¼',
        'á»e': 'oáº¹', 'á»Œe': 'Oáº¹', 'á»ŒE': 'Oáº¸',
        'Ã¹y': 'uá»³', 'Ã™y': 'Uá»³', 'Ã™Y': 'Uá»²',
        'Ãºy': 'uÃ½', 'Ãšy': 'UÃ½', 'ÃšY': 'UÃ',
        'á»§y': 'uá»·', 'á»¦y': 'Uá»·', 'á»¦Y': 'Uá»¶',
        'Å©y': 'uá»¹', 'Å¨y': 'Uá»¹', 'Å¨Y': 'Uá»¸',
        'á»¥y': 'uá»µ', 'á»¤y': 'Uá»µ', 'á»¤Y': 'Uá»´',
    }
    
    @staticmethod
    def normalize_unicode(text):
        char1252 = r'aÌ€|aÌ|aÌ‰|aÌƒ|aÌ£|Ã¢Ì€|Ã¢Ì|Ã¢Ì‰|Ã¢Ìƒ|Ã¢Ì£|ÄƒÌ€|ÄƒÌ|ÄƒÌ‰|ÄƒÌƒ|ÄƒÌ£|eÌ€|eÌ|eÌ‰|eÌƒ|eÌ£|ÃªÌ€|ÃªÌ|ÃªÌ‰|ÃªÌƒ|ÃªÌ£|iÌ€|iÌ|iÌ‰|iÌƒ|iÌ£|oÌ€|oÌ|oÌ‰|oÌƒ|oÌ£|Ã´Ì€|Ã´Ì|Ã´Ì‰|Ã´Ìƒ|Ã´Ì£|Æ¡Ì€|Æ¡Ì|Æ¡Ì‰|Æ¡Ìƒ|Æ¡Ì£|uÌ€|uÌ|uÌ‰|uÌƒ|uÌ£|Æ°Ì€|Æ°Ì|Æ°Ì‰|Æ°Ìƒ|Æ°Ì£|yÌ€|yÌ|yÌ‰|yÌƒ|yÌ£|AÌ€|AÌ|AÌ‰|AÌƒ|AÌ£|Ã‚Ì€|Ã‚Ì|Ã‚Ì‰|Ã‚Ìƒ|Ã‚Ì£|Ä‚Ì€|Ä‚Ì|Ä‚Ì‰|Ä‚Ìƒ|Ä‚Ì£|EÌ€|EÌ|EÌ‰|EÌƒ|EÌ£|ÃŠÌ€|ÃŠÌ|ÃŠÌ‰|ÃŠÌƒ|ÃŠÌ£|IÌ€|IÌ|IÌ‰|IÌƒ|IÌ£|OÌ€|OÌ|OÌ‰|OÌƒ|OÌ£|Ã”Ì€|Ã”Ì|Ã”Ì‰|Ã”Ìƒ|Ã”Ì£|Æ Ì€|Æ Ì|Æ Ì‰|Æ Ìƒ|Æ Ì£|UÌ€|UÌ|UÌ‰|UÌƒ|UÌ£|Æ¯Ì€|Æ¯Ì|Æ¯Ì‰|Æ¯Ìƒ|Æ¯Ì£|YÌ€|YÌ|YÌ‰|YÌƒ|YÌ£'
        charutf8 = r'Ã |Ã¡|áº£|Ã£|áº¡|áº§|áº¥|áº©|áº«|áº­|áº±|áº¯|áº³|áºµ|áº·|Ã¨|Ã©|áº»|áº½|áº¹|á»|áº¿|á»ƒ|á»…|á»‡|Ã¬|Ã­|á»‰|Ä©|á»‹|Ã²|Ã³|á»|Ãµ|á»|á»“|á»‘|á»•|á»—|á»™|á»|á»›|á»Ÿ|á»¡|á»£|Ã¹|Ãº|á»§|Å©|á»¥|á»«|á»©|á»­|á»¯|á»±|á»³|Ã½|á»·|á»¹|á»µ|Ã€|Ã|áº¢|Ãƒ|áº |áº¦|áº¤|áº¨|áºª|áº¬|áº°|áº®|áº²|áº´|áº¶|Ãˆ|Ã‰|áºº|áº¼|áº¸|á»€|áº¾|á»‚|á»„|á»†|ÃŒ|Ã|á»ˆ|Ä¨|á»Š|Ã’|Ã“|á»|Ã•|á»Œ|á»’|á»|á»”|á»–|á»˜|á»œ|á»š|á»|á» |á»¢|Ã™|Ãš|á»¦|Å¨|á»¤|á»ª|á»¨|á»¬|á»®|á»°|á»²|Ã|á»¶|á»¸|á»´'
        char_map = dict(zip(char1252.split('|'), charutf8.split('|')))
        return re.sub(char1252, lambda x: char_map[x.group()], text.strip())

    @staticmethod
    def normalize_typing(text):
        for wrong_word, correct_word in VietnameseNormalizer.VINAI_NORMALIZED_TONE.items():
            text = text.replace(wrong_word, correct_word)
        return text.strip()

class VietnameseCleaner:
    def remove_emoji(text):
        emoji_pattern = re.compile("["
            u"\U0001F600-\U0001F64F"  # emoticons
            u"\U0001F300-\U0001F5FF"  # symbols & pictographs
            u"\U0001F680-\U0001F6FF"  # transport & map symbols
            u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
            u"\U00002500-\U00002BEF"  # chinese char
            u"\U00002702-\U000027B0"
            u"\U000024C2-\U0001F251"
            u"\U0001f926-\U0001f937"
            u"\U00010000-\U0010ffff"
                            "]+", flags=re.UNICODE)
        return emoji_pattern.sub(r'', text)

    def remove_punctuation_emoji(text):
        # XÃ³a cÃ¡c emoji: :))), :(((, =)))), =(((, ...
        text = re.sub(r'[:;=xX8@â‚«&]+-?[)(DPpOo3v]+', '', text)
        text = re.sub(r'[)(DPpOo3v]+-?[:;=xX8@â‚«&]+', '', text)
        text = re.sub(r'[:;=xX8@â‚«&]+[)(DPpOo3v]+', '', text)
        
        # XÃ³a cÃ¡c emoji @@, @.@, =.=, ...
        text = re.sub(r'[@=^~*]([.o_-])?[@=^~*]', '', text)
        
        # XÃ³a ngoáº·c thá»«a
        text = re.sub(r'\(\)', '', text)
        return text.strip()

    def remove_uncharacter_Vietnamese(text):
        ALLOWED_PUNCTUATION = r'\.,!?â€“:;\''
        VN_CHARS = 'Ã¡Ã áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã©Ã¨áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã³Ã²á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£Ã­Ã¬á»‰Ä©á»‹ÃºÃ¹á»§Å©á»¥Æ°á»©á»«á»­á»¯á»±Ã½á»³á»·á»¹á»µÄ‘ÃÃ€áº¢Ãƒáº Ä‚áº®áº°áº²áº´áº¶Ã‚áº¤áº¦áº¨áºªáº¬Ã‰Ãˆáººáº¼áº¸ÃŠáº¾á»€á»‚á»„á»†Ã“Ã’á»Ã•á»ŒÃ”á»á»’á»”á»–á»˜Æ á»šá»œá»á» á»¢ÃÃŒá»ˆÄ¨á»ŠÃšÃ™á»¦Å¨á»¤Æ¯á»¨á»ªá»¬á»®á»°Ãá»²á»¶á»¸á»´Ä'
        text = re.sub(fr'[^\sa-zA-Z0-9{VN_CHARS}{ALLOWED_PUNCTUATION}]', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        return text

    @staticmethod
    def clean_social_text(text):
        text = VietnameseCleaner.remove_emoji(text)
        text = VietnameseCleaner.remove_punctuation_emoji(text)
        
        # remove html
        text = re.sub(r'<[^>]*>', '', text)
        
        # remove hashtag
        text = re.sub(r'#\w+', '', text)
        
        # remove url
        text = re.sub(r'https?://\S+|www\.\S+', '', text)
        
        # remove hotline
        text = re.sub(r'\b[(]?(\+84|0)[)]?\d{3}[-\s\.]?\d{3}[-\s\.]?\d{3,6}\b', '', text)
        
        # remove email
        text = re.sub(r'[^@ \t\r\n]+@[^@ \t\r\n]+\.[^@ \t\r\n]+', '', text)
        
        # remove repeated characters (giáº£m bá»›t cÆ°á»ng Ä‘á»™ cá»§a tá»«)
        text = re.sub(r'(.)\1{2,}', r'\1\1', text)

        # remove uncharacter + extra whitespace
        text = VietnameseCleaner.remove_uncharacter_Vietnamese(text)
        return text


class VietnameseTextProcessor:
    def __init__(self, max_correction_length=512):
        self.max_correction_length = max_correction_length
        self._build_teencodes()
    
    def _build_teencodes(self):
        self.teencodes = {
            'ok': ['okie', 'okey', 'Ã´kÃª', 'oki', 'oke', 'okay', 'okÃª'], 
            'khÃ´ng': ['kg', 'not', 'k', 'kh', 'kÃ´', 'hok', 'ko', 'khong'], 'khÃ´ng pháº£i': ['kp'], 
            'cáº£m Æ¡n': ['tks', 'thks', 'thanks', 'ths', 'thank'], 'há»“i Ä‘Ã³': ['hÃ¹i Ä‘Ã³'], 'muá»‘n': ['mÃºn'],
            
            'ráº¥t tá»‘t': ['perfect', 'â¤ï¸', 'ğŸ˜'], 'dá»… thÆ°Æ¡ng': ['cute'], 'yÃªu': ['iu'], 'thÃ­ch': ['thik'], 
            'tá»‘t': [
                'gud', 'good', 'gÃºt', 'tot', 'nice',
                'hehe', 'hihi', 'haha', 'hjhj', 'thick', '^_^', ':)', '=)'
                'ğŸ‘', 'ğŸ‰', 'ğŸ˜€', 'ğŸ˜‚', 'ğŸ¤—', 'ğŸ˜™', 'ğŸ™‚'
            ], 
            'bÃ¬nh thÆ°á»ng': ['bt', 'bthg'], 
            'khÃ´ng tá»‘t':  ['lol', 'cc', 'huhu', ':(', 'ğŸ˜”', 'ğŸ˜“'],
            'tá»‡': ['sad', 'por', 'poor', 'bad'], 'giáº£ máº¡o': ['fake'], 
            
            'quÃ¡': ['wa', 'wÃ¡', 'qÃ¡'], 'Ä‘Æ°á»£c': ['Ä‘x', 'dk', 'dc', 'Ä‘k', 'Ä‘c'], 
            'vá»›i': ['vs'], 'gÃ¬': ['j'], 'rá»“i': ['r'], 'mÃ¬nh': ['m', 'mik', 'mk'], 
            'thá»i gian': ['time'], 'giá»': ['h'],
            'chÄƒm sÃ³c': ['takecere', 'takecare', 'take care'],
            'homestay': ['homstay', 'home stay'],
            'cÃ  phÃª': ['cf'],
            'nhÃ¢n viÃªn': ['nv'],
            'hÆ°á»›ng dáº«n viÃªn': ['hdv'],
            'chá»©ng minh nhÃ¢n dÃ¢n': ['cmnd'],
            'cÄƒn cÆ°á»›c cÃ´ng dÃ¢n': ['cccd'],
            'giáº¥y phÃ©p lÃ¡i xe': ['gplx'],
            'giÃ¡ cáº£': ['giÃ¡ cÃ£'],
            'hÃ ng': ['hÃ g'],
            'cá»™c cáº±n': ['cá»c cáº±n'],
            'buffet': ['buffer', 'bÃºp phÃª'],
            'máº¥t': ['mÃ¢t'],
            'nhÃ  vá»‡ sinh': ['wc'],
            'siÃªu': ['xiu']
        }
                
        self.teencodes = {word: key for key, values in self.teencodes.items() for word in values}
        teencode_url = 'https://gist.githubusercontent.com/behitek/7d9441c10b3c2739499fc5a4d9ea06fb/raw/df939245b3e841b62af115be4dcb3516dadc9fc5/teencode.txt'
        response = requests.get(teencode_url)
        
        if response.status_code == 200:
            text_data = StringIO(response.text)
            for pair in text_data:
                teencode, true_text = pair.split('\t')
                self.teencodes[teencode.strip()] = true_text.strip()
            self.teencodes = {k: self.teencodes[k] for k in sorted(self.teencodes)}
        else: print('Failed to fetch teencode.txt from', teencode_url)
        
    def correct_vietnamese_errors(self, texts):
        # https://huggingface.co/bmd1905/vietnamese-correction
        predictions = self.corrector(texts, max_length=self.max_correction_length, truncation=True)
        return [prediction['generated_text'] for prediction in predictions]
    
    def normalize_teencodes(self, text):
        def replace_teencode(word):
            return self.teencodes.get(word, word)
        
        tokens = re.split(r'(\s+)', text)
        tokens = [replace_teencode(token) if not token.isspace() else token for token in tokens]
        return ''.join(tokens)
    
    def process_text(self, text, normalize_tone=True):
        if normalize_tone:
            text = VietnameseNormalizer.normalize_unicode(text)
            text = VietnameseNormalizer.normalize_typing(text)
        text = VietnameseCleaner.clean_social_text(text)
        text = self.normalize_teencodes(text.lower())
        return text.strip()

if __name__ == '__main__':
    processor = VietnameseTextProcessor()
    text = "KhÃ¡ch sáº¡n mÆ¡Ìi xÃ¢y, khÃ´ng  khÃ­ trong laÌ€nh. Ngay phÃ´Ì Äƒn váº·t nhÃ  ChungğŸ‘.\nCuoc song tuyet voi"
    print(f'Original text: {text}\n')
    new_text = processor.process_text(text)
    print(f'New text: {new_text}')