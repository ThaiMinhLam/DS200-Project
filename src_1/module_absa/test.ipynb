{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e6499c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from io import StringIO\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyCIFghSOEJMA8kPzIP7n40OyCwFYGVaanc\"\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e28fcb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VietnameseNormalizer:\n",
    "    \"\"\"\n",
    "    Tham khảo: https://github.com/VinAIResearch/BARTpho/blob/main/VietnameseToneNormalization.md\n",
    "    \"\"\"\n",
    "    VINAI_NORMALIZED_TONE = {\n",
    "        'òa': 'oà', 'Òa': 'Oà', 'ÒA': 'OÀ', \n",
    "        'óa': 'oá', 'Óa': 'Oá', 'ÓA': 'OÁ', \n",
    "        'ỏa': 'oả', 'Ỏa': 'Oả', 'ỎA': 'OẢ',\n",
    "        'õa': 'oã', 'Õa': 'Oã', 'ÕA': 'OÃ',\n",
    "        'ọa': 'oạ', 'Ọa': 'Oạ', 'ỌA': 'OẠ',\n",
    "        'òe': 'oè', 'Òe': 'Oè', 'ÒE': 'OÈ',\n",
    "        'óe': 'oé', 'Óe': 'Oé', 'ÓE': 'OÉ',\n",
    "        'ỏe': 'oẻ', 'Ỏe': 'Oẻ', 'ỎE': 'OẺ',\n",
    "        'õe': 'oẽ', 'Õe': 'Oẽ', 'ÕE': 'OẼ',\n",
    "        'ọe': 'oẹ', 'Ọe': 'Oẹ', 'ỌE': 'OẸ',\n",
    "        'ùy': 'uỳ', 'Ùy': 'Uỳ', 'ÙY': 'UỲ',\n",
    "        'úy': 'uý', 'Úy': 'Uý', 'ÚY': 'UÝ',\n",
    "        'ủy': 'uỷ', 'Ủy': 'Uỷ', 'ỦY': 'UỶ',\n",
    "        'ũy': 'uỹ', 'Ũy': 'Uỹ', 'ŨY': 'UỸ',\n",
    "        'ụy': 'uỵ', 'Ụy': 'Uỵ', 'ỤY': 'UỴ',\n",
    "    }\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_unicode(text):\n",
    "        char1252 = r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'\n",
    "        charutf8 = r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'\n",
    "        char_map = dict(zip(char1252.split('|'), charutf8.split('|')))\n",
    "        return re.sub(char1252, lambda x: char_map[x.group()], text.strip())\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_typing(text):\n",
    "        for wrong_word, correct_word in VietnameseNormalizer.VINAI_NORMALIZED_TONE.items():\n",
    "            text = text.replace(wrong_word, correct_word)\n",
    "        return text.strip()\n",
    "\n",
    "class VietnameseCleaner:\n",
    "    def remove_emoji(text):\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "            u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "            u\"\\U00002702-\\U000027B0\"\n",
    "            u\"\\U000024C2-\\U0001F251\"\n",
    "            u\"\\U0001f926-\\U0001f937\"\n",
    "            u\"\\U00010000-\\U0010ffff\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"\n",
    "            u\"\\U0001F300-\\U0001F5FF\"\n",
    "            u\"\\U0001F680-\\U0001F6FF\"\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "            u\"\\U00002700-\\U000027BF\"\n",
    "            u\"\\U000024C2-\\U0001F251\"            \n",
    "                            \"]+\", flags=re.UNICODE)\n",
    "        return emoji_pattern.sub(r'', text)\n",
    "\n",
    "    def remove_punctuation_emoji(text):\n",
    "        # Xóa các emoji: :))), :(((, =)))), =(((, ...\n",
    "        text = re.sub(r'[:;=xX8@₫&]+-?[)(DPpOo3v]+', '', text)\n",
    "        text = re.sub(r'[)(DPpOo3v]+-?[:;=xX8@₫&]+', '', text)\n",
    "        text = re.sub(r'[:;=xX8@₫&]+[)(DPpOo3v]+', '', text)\n",
    "        \n",
    "        # Xóa các emoji @@, @.@, =.=, ...\n",
    "        text = re.sub(r'[@=^~*]([.o_-])?[@=^~*]', '', text)\n",
    "        \n",
    "        # Xóa ngoặc thừa\n",
    "        text = re.sub(r'\\(\\)', '', text)\n",
    "        return text.strip()\n",
    "\n",
    "    def remove_uncharacter_Vietnamese(text):\n",
    "        ALLOWED_PUNCTUATION = r'\\.,!?–:;'\n",
    "        VN_CHARS = 'áàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđÁÀẢÃẠĂẮẰẲẴẶÂẤẦẨẪẬÉÈẺẼẸÊẾỀỂỄỆÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢÍÌỈĨỊÚÙỦŨỤƯỨỪỬỮỰÝỲỶỸỴĐ'\n",
    "        text = re.sub(fr'[^\\sa-zA-Z0-9{VN_CHARS}{ALLOWED_PUNCTUATION}]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_social_text(text):\n",
    "        text = VietnameseCleaner.remove_emoji(text)\n",
    "        text = VietnameseCleaner.remove_punctuation_emoji(text)\n",
    "        \n",
    "        # remove html\n",
    "        text = re.sub(r'<[^>]*>', '', text)\n",
    "        \n",
    "        # remove hashtag\n",
    "        text = re.sub(r'#\\w+', '', text)\n",
    "        \n",
    "        # remove url\n",
    "        text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "        \n",
    "        # remove hotline\n",
    "        text = re.sub(r'\\b[(]?(\\+84|0)[)]?\\d{3}[-\\s\\.]?\\d{3}[-\\s\\.]?\\d{3,6}\\b', '', text)\n",
    "        \n",
    "        # remove email\n",
    "        text = re.sub(r'[^@ \\t\\r\\n]+@[^@ \\t\\r\\n]+\\.[^@ \\t\\r\\n]+', '', text)\n",
    "        \n",
    "        # remove repeated characters (giảm bớt cường độ của từ)\n",
    "        text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
    "\n",
    "        # remove uncharacter + extra whitespace\n",
    "        text = VietnameseCleaner.remove_uncharacter_Vietnamese(text)\n",
    "        return text\n",
    "\n",
    "\n",
    "class VietnameseTextProcessor:\n",
    "    def __init__(self, max_correction_length=512):\n",
    "        self.max_correction_length = max_correction_length\n",
    "        self._build_teencodes()\n",
    "    \n",
    "    def _build_teencodes(self):\n",
    "        self.teencodes = {\n",
    "            'ok': ['okie', 'okey', 'ôkê', 'oki', 'oke', 'okay', 'okê'], \n",
    "            'không': ['kg', 'not', 'k', 'kh', 'kô', 'hok', 'ko', 'khong'], 'không phải': ['kp'], \n",
    "            'cảm ơn': ['tks', 'thks', 'thanks', 'ths', 'thank'], 'hồi đó': ['hùi đó'], 'muốn': ['mún'],\n",
    "            \n",
    "            'rất tốt': ['perfect', '❤️', '😍'], 'dễ thương': ['cute'], 'yêu': ['iu'], 'thích': ['thik'], \n",
    "            'tốt': [\n",
    "                'gud', 'good', 'gút', 'tot', 'nice',\n",
    "                'hehe', 'hihi', 'haha', 'hjhj', 'thick', '^_^', ':)', '=)'\n",
    "                '👍', '🎉', '😀', '😂', '🤗', '😙', '🙂'\n",
    "            ], \n",
    "            'bình thường': ['bt', 'bthg'], 'hàg': ['hàng'], \n",
    "            'không tốt':  ['lol', 'cc', 'huhu', ':(', '😔', '😓'],\n",
    "            'tệ': ['sad', 'por', 'poor', 'bad'], 'giả mạo': ['fake'], \n",
    "            \n",
    "            'quá': ['wa', 'wá', 'qá'], 'được': ['đx', 'dk', 'dc', 'đk', 'đc'], \n",
    "            'với': ['vs'], 'gì': ['j'], 'rồi': ['r'], 'mình': ['m', 'mik'], \n",
    "            'thời gian': ['time'], 'giờ': ['h'], \n",
    "        }\n",
    "                \n",
    "        self.teencodes = {word: key for key, values in self.teencodes.items() for word in values}\n",
    "        teencode_url = 'https://gist.githubusercontent.com/behitek/7d9441c10b3c2739499fc5a4d9ea06fb/raw/df939245b3e841b62af115be4dcb3516dadc9fc5/teencode.txt'\n",
    "        response = requests.get(teencode_url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            text_data = StringIO(response.text)\n",
    "            for pair in text_data:\n",
    "                teencode, true_text = pair.split('\\t')\n",
    "                self.teencodes[teencode.strip()] = true_text.strip()\n",
    "            self.teencodes = {k: self.teencodes[k] for k in sorted(self.teencodes)}\n",
    "        else: print('Failed to fetch teencode.txt from', teencode_url)\n",
    "        \n",
    "    def correct_vietnamese_errors(self, texts):\n",
    "        # https://huggingface.co/bmd1905/vietnamese-correction\n",
    "        predictions = self.corrector(texts, max_length=self.max_correction_length, truncation=True)\n",
    "        return [prediction['generated_text'] for prediction in predictions]\n",
    "    \n",
    "    def normalize_teencodes(self, text):\n",
    "        words = []\n",
    "        for word in text.split():\n",
    "            words.append(self.teencodes.get(word, word))\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    def process_text(self, text, normalize_tone=True):\n",
    "        # text = text.lower()\n",
    "        if normalize_tone:\n",
    "            text = VietnameseNormalizer.normalize_unicode(text)\n",
    "            text = VietnameseNormalizer.normalize_typing(text)\n",
    "        text = VietnameseCleaner.clean_social_text(text)\n",
    "        text = self.normalize_teencodes(text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "986a7f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aspect_dict cho ABSA\n",
    "ASPECT_DICT = {\n",
    "    \"HOTELS\": [\n",
    "        \"HOTEL#LOCATION\", \"HOTEL#QUALITY\", \"HOTEL#FACILITIES\", \"HOTEL#STYLE\",\n",
    "        \"WIFI\", \"PRICE\", \"ROOM#QUALITY\", \"ROOM#STYLE\", \"ROOM#FACILITIES\",\n",
    "        \"ROOM#SOUND\", \"ROOM#VIEW\", \"ROOM#ATMOSPHERE\", \"ROOM#CLEANLINESS\",\n",
    "        \"SERVICE#STAFF\", \"SERVICE#CHECKIN\"\n",
    "    ],\n",
    "    \"RESTAURANTS\": [\n",
    "        \"LOCATION\", \"PRICE\", \"FOOD#QUALITY\", \"FOOD#VARIETY\",\n",
    "        \"FOOD#PRESENTATION\", \"FOOD#FRESHNESS\", \"DRINK#QUALITY\",\n",
    "        \"ENVIRONMENT#CLEANLINESS\", \"ENVIRONMENT#AMBIENCE\",\n",
    "        \"SERVICE#STAFF\", \"SERVICE#ORDER\"\n",
    "    ],\n",
    "    \"DRINKPLACES\": [\n",
    "        \"LOCATION\", \"PRICE\", \"FOOD#QUALITY\", \"DRINK#QUALITY\",\n",
    "        \"DRINK#VARIETY\", \"ENVIRONMENT#CLEANLINESS\", \"ENVIRONMENT#AMBIENCE\",\n",
    "        \"SERVICE#STAFF\", \"SERVICE#ORDER\"\n",
    "    ],\n",
    "    \"EATERY\": [\n",
    "        \"LOCATION\", \"PRICE\", \"FOOD#QUALITY\", \"FOOD#VARIETY\",\n",
    "        \"DRINK#QUALITY\", \"DRINK#VARIETY\", \"ENVIRONMENT#CLEANLINESS\",\n",
    "        \"ENVIRONMENT#AMBIENCE\", \"SERVICE#STAFF\", \"SERVICE#ORDER\"\n",
    "    ],\n",
    "    \"ATTRACTIONS\": [\n",
    "        \"LOCATION\", \"PRICE\", \"SERVICE#STAFF\", \"SERVICE#BOOKING\",\n",
    "        \"ENVIRONMENT#SCENERY\", \"ENVIRONMENT#ATMOSPHERE\",\n",
    "        \"EXPERIENCE#ACTIVITY\"\n",
    "    ],\n",
    "    \"RENTALSERVICES\": [\n",
    "        \"LOCATION\", \"PRICE\", \"SERVICE#RENTING\", \"SERVICE#STAFF\",\n",
    "        \"VEHICLE#QUALITY\"\n",
    "    ],\n",
    "    \"TOUR\": [\n",
    "        \"LOCATION\", \"PRICE\", \"SERVICE#STAFF\", \"EXPERIENCE#ACTIVITY\",\n",
    "        \"ENVIRONMENT#SCENERY\", \"ENVIRONMENT#ATMOSPHERE\"\n",
    "    ],\n",
    "    \"CAMPING\": [\n",
    "        \"LOCATION#DISTANCE\", \"LOCATION#ACCESSIBILITY\", \"SERVICE#STAFF\",\n",
    "        \"ENVIRONMENT#SCENERY\", \"ENVIRONMENT#WEATHER\", \"ENVIRONMENT#ATMOSPHERE\"\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de729c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "def build_absa_prompt_template(domain: str) -> PromptTemplate:\n",
    "    # if domain not in ASPECT_DICT:\n",
    "    #     raise ValueError(f\"Domain '{domain}' không tồn tại. Chọn từ: {list(ASPECT_DICT.keys())}\")\n",
    "\n",
    "    # aspects = ASPECT_DICT[domain]\n",
    "    # aspect_list_str = \"\\n- \".join(aspects)\n",
    "\n",
    "    template_text = f\"\"\"\n",
    "Bạn là công cụ gán nhãn Aspect-Based Sentiment Analysis (ABSA) cho **review tiếng Việt** về dịch vụ thuộc lĩnh vực **{domain}** tại Việt Nam.\n",
    "Nhiệm vụ:\n",
    "- Xác định các aspect xuất hiện trong review dưới đây và gán sentiment phù hợp.\n",
    "- Chỉ sử dụng **danh sách aspect dưới đây**, không tự nghĩ thêm aspect khác.\n",
    "- Sentiment chỉ được gán là: \"POSITIVE\", \"NEUTRAL\", \"NEGATIVE\".\n",
    "\n",
    "### Danh sách aspect:\n",
    "- {ASPECT_DICT}\n",
    "\n",
    "### Quy định output:\n",
    "- Chỉ trả về **dạng list JSON**, mỗi phần tử gồm:\n",
    "{{\n",
    "    \"aspect\": \"ASPECT_NAME\",\n",
    "    \"sentiment\": \"POSITIVE/NEUTRAL/NEGATIVE\"\n",
    "}}\n",
    "- Nếu không có aspect nào, trả về: []\n",
    "\n",
    "### Review:\n",
    "{{text}}\n",
    "\"\"\"\n",
    "    return PromptTemplate(\n",
    "        input_variables=[\"text\"],\n",
    "        template=template_text\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ca57b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ABSAExtractor:\n",
    "    def __init__(self, llm):\n",
    "        self.prompt_template = PromptTemplate(\n",
    "            input_variables=[\"text\", \"domain\", \"aspects\"],\n",
    "            template=\"\"\"\n",
    "Bạn là công cụ Aspect-Based Sentiment Analysis (ABSA) cho **review tiếng Việt** về dịch vụ trong lĩnh vực **{domain}** tại Đà Lạt, Việt Nam.\n",
    "Nhiệm vụ:\n",
    "- Xác định các aspect xuất hiện trong review dưới đây và gán sentiment phù hợp.\n",
    "- Chỉ sử dụng **danh sách aspect dưới đây**, không tự nghĩ thêm aspect khác.\n",
    "- Sentiment chỉ được gán: \"POSITIVE\", \"NEUTRAL\", \"NEGATIVE\".\n",
    "\n",
    "### Danh sách aspect:\n",
    "{aspects}\n",
    "\n",
    "### Quy định output:\n",
    "- Chỉ trả về **dạng list JSON**, mỗi phần tử gồm:\n",
    "{{\"aspect\": \"ASPECT_NAME\", \"sentiment\": \"POSITIVE/NEUTRAL/NEGATIVE\"}}\n",
    "- Nếu không có aspect nào thì trả về: []\n",
    "\n",
    "### Review:\n",
    "{text}\n",
    "\"\"\"\n",
    "        )\n",
    "        self.chain = LLMChain(llm=llm, prompt=self.prompt_template)\n",
    "\n",
    "    def extract(self, text: str, domain: str, aspects_list: list, parse_json: bool = True):\n",
    "        \"\"\"\n",
    "        text: câu review cần gán ABSA\n",
    "        domain: tên domain (ví dụ: HOTELS)\n",
    "        aspects_list: list các aspect (sẽ convert sang dạng string phù hợp)\n",
    "        parse_json: nếu True, tự parse JSON trả về, nếu lỗi thì trả raw string\n",
    "        \"\"\"\n",
    "        aspects_formatted = \"\\n\".join(f\"- {aspect}\" for aspect in aspects_list)\n",
    "\n",
    "        result = self.chain.run(\n",
    "            text=text,\n",
    "            domain=domain,\n",
    "            aspects=aspects_formatted\n",
    "        )\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "639d3200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "[\n",
      "  {\"aspect\": \"ROOM#CLEANLINESS\", \"sentiment\": \"POSITIVE\"},\n",
      "  {\"aspect\": \"ROOM#VIEW\", \"sentiment\": \"POSITIVE\"},\n",
      "  {\"aspect\": \"SERVICE#STAFF\", \"sentiment\": \"POSITIVE\"},\n",
      "  {\"aspect\": \"PRICE\", \"sentiment\": \"NEGATIVE\"}\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\", temperature=0)\n",
    "\n",
    "absa_extractor = ABSAExtractor(llm)\n",
    "\n",
    "text = \"Phòng sạch, view đẹp, nhân viên vui vẻ, giá hơi cao.\"\n",
    "domain = \"HOTELS\"\n",
    "aspect_list = ASPECT_DICT[domain]\n",
    "result = absa_extractor.extract(text=text, domain=domain, aspects_list=aspect_list)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4c9e724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_result(result):\n",
    "    result = result.strip()\n",
    "    if result.startswith(\"```\"):\n",
    "        # Loại bỏ dòng đầu ```json hoặc ```\n",
    "        lines = result.split(\"\\n\")\n",
    "        if lines[0].startswith(\"```\"):\n",
    "            lines = lines[1:]\n",
    "        # Loại bỏ dòng cuối ```\n",
    "        if lines[-1].startswith(\"```\"):\n",
    "            lines = lines[:-1]\n",
    "        result = \"\\n\".join(lines).strip()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "80b5563d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = clean_result(result)\n",
    "temp2 = json.loads(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d7057dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "35d7ba1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'aspect': 'ROOM#CLEANLINESS', 'sentiment': 'POSITIVE'},\n",
       " {'aspect': 'ROOM#VIEW', 'sentiment': 'POSITIVE'},\n",
       " {'aspect': 'SERVICE#STAFF', 'sentiment': 'POSITIVE'},\n",
       " {'aspect': 'PRICE', 'sentiment': 'NEGATIVE'}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a09d7e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
