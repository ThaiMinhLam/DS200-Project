{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60c7cdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c70c724",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer, KafkaProducer\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "import pickle\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(r'F:\\Studies\\Third_year\\Big_data\\Final_Code')\n",
    "from final_src.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72b647ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import final_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aec4e6b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-5DQQ5LDG.lan:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>NER_ABSA_Streaming</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2c23ab0e510>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scala_version = '2.12'\n",
    "spark_version = '3.5.5'\n",
    "\n",
    "packages = [\n",
    "    f'org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version}',\n",
    "    'org.apache.kafka:kafka-clients:2.8.0'\n",
    "]\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"NER_ABSA_Streaming\")\\\n",
    "                    .config(\"spark.jars.packages\", \",\".join(packages)) \\\n",
    "                    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "                    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "                    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "011d22e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "drinkplaces_aspects = ['DRINK#QUALITY',\n",
    " 'DRINK#VARIETY',\n",
    " 'ENVIRONMENT#CLEANLINESS',\n",
    " 'ENVIRONMENT#AMBIENCE',\n",
    " 'FOOD#QUALITY',\n",
    " 'LOCATION',\n",
    " 'PRICE',\n",
    " 'SERVICE#ORDER',\n",
    " 'SERVICE#STAFF']\n",
    "\n",
    "hotels_aspects = ['HOTEL#LOCATION',\n",
    " 'HOTEL#QUALITY',\n",
    " 'HOTEL#FACILITIES',\n",
    " 'HOTEL#STYLE',\n",
    " 'WIFI',\n",
    " 'PRICE',\n",
    " 'ROOM#QUALITY',\n",
    " 'ROOM#STYLE',\n",
    " 'ROOM#FACILITIES',\n",
    " 'ROOM#SOUND',\n",
    " 'ROOM#VIEW',\n",
    " 'ROOM#ATMOSPHERE',\n",
    " 'ROOM#CLEANLINESS',\n",
    " 'SERVICE#STAFF',\n",
    " 'SERVICE#CHECKIN']\n",
    "\n",
    "restaurants_aspects = ['LOCATION',\n",
    " 'PRICE',\n",
    " 'FOOD#QUALITY',\n",
    " 'FOOD#VARIETY',\n",
    " 'FOOD#PRESENTATION',\n",
    " 'FOOD#FRESHNESS',\n",
    " 'DRINK#QUALITY',\n",
    " 'ENVIRONMENT#CLEANLINESS',\n",
    " 'ENVIRONMENT#AMBIENCE',\n",
    " 'SERVICE#STAFF',\n",
    " 'SERVICE#ORDER']\n",
    "\n",
    "eateries_aspects = ['LOCATION',\n",
    " 'PRICE',\n",
    " 'FOOD#QUALITY',\n",
    " 'FOOD#VARIETY',\n",
    " 'DRINK#QUALITY',\n",
    " 'DRINK#VARIETY',\n",
    " 'ENVIRONMENT#CLEANLINESS',\n",
    " 'ENVIRONMENT#AMBIENCE',\n",
    " 'SERVICE#STAFF',\n",
    " 'SERVICE#ORDER']\n",
    "\n",
    "attractions_aspects = ['LOCATION',\n",
    " 'PRICE',\n",
    " 'SERVICE#STAFF',\n",
    " 'ENVIRONMENT#SCENERY',\n",
    " 'ENVIRONMENT#ATMOSPHERE',\n",
    " 'EXPERIENCE#ACTIVITY']\n",
    "\n",
    "rents_aspects = ['LOCATION', 'PRICE', 'SERVICE#RENTING', 'SERVICE#STAFF', 'VEHICLE#QUALITY']\n",
    "\n",
    "tours_aspects = ['LOCATION',\n",
    " 'PRICE',\n",
    " 'SERVICE#STAFF',\n",
    " 'EXPERIENCE#ACTIVITY',\n",
    " 'ENVIRONMENT#SCENERY',\n",
    " 'ENVIRONMENT#ATMOSPHERE']\n",
    "\n",
    "campings_aspects = ['LOCATION#DISTANCE',\n",
    " 'LOCATION#ACCESSIBILITY',\n",
    " 'SERVICE#STAFF',\n",
    " 'ENVIRONMENT#SCENERY',\n",
    " 'ENVIRONMENT#WEATHER',\n",
    " 'ENVIRONMENT#ATMOSPHERE']\n",
    "\n",
    "sentiment_map = {\n",
    "    1: 'NEGATIVE',\n",
    "    2: 'NEUTRAL',\n",
    "    3: 'POSITIVE'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ed87764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "label_list = ['B-TOUR', 'B-RENT', 'B-RESTAURANT', 'I-RESTAURANT', 'B-ATTRACTION', 'I-TOUR', 'I-EATERY', 'O', 'B-HOTEL', 'I-HOTEL', 'I-ATTRACTION', 'I-CAMPING', 'B-EATERY', 'B-DRINKPLACE', 'I-RENT', 'B-CAMPING', 'I-DRINKPLACE']\n",
    "label_map = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for i, label in enumerate(label_list)}\n",
    "\n",
    "def merge_subwords(tokens, labels):\n",
    "    merged_tokens = []\n",
    "    merged_labels = []\n",
    "\n",
    "    current_token = \"\"\n",
    "    current_label = None\n",
    "\n",
    "    for token, label in zip(tokens, labels):\n",
    "        if token.endswith('@@'):  # Subword token\n",
    "            current_token += token[:-2]  # Bỏ @@ và nối vào\n",
    "            if current_label is None:\n",
    "                current_label = label  # Lấy label đầu tiên (B- hoặc I-)\n",
    "        else:\n",
    "            current_token += token  # Token đầy đủ, nối vào\n",
    "            if current_label is None:\n",
    "                current_label = label\n",
    "            merged_tokens.append(current_token)\n",
    "            merged_labels.append(current_label)\n",
    "            current_token = \"\"\n",
    "            current_label = None\n",
    "\n",
    "    # Nếu còn token cuối\n",
    "    if current_token:\n",
    "        merged_tokens.append(current_token)\n",
    "        merged_labels.append(current_label)\n",
    "\n",
    "    return merged_tokens, merged_labels\n",
    "\n",
    "\n",
    "def predict_ner(text, model, tokenizer, id2label):\n",
    "    # Tokenize\n",
    "    # text = text.lower()\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        is_split_into_words=False\n",
    "    )\n",
    "\n",
    "    # Predict\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    # Decode tokens và labels\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    labels = [id2label[label_id.item()] for label_id in predictions[0]]\n",
    "    # print(labels)\n",
    "    new_tokens, new_lables = merge_subwords(tokens[1:-1], labels[1:-1])\n",
    "    # Hiển thị sạch\n",
    "    # for token, label in zip(new_tokens, new_lables):\n",
    "    #     # token_clean = token.replace(\"▁\", \"\") if \"▁\" in token else token\n",
    "    #     # print(f\"{token_clean}\\t{label}\")\n",
    "    #     print(f\"{token}\\t{label}\")\n",
    "    return new_tokens, new_lables\n",
    "\n",
    "def extract_location_and_domain(tokens, labels):\n",
    "    target_labels = ['B-TOUR', 'B-RENT', 'B-RESTAURANT', \n",
    "                     'B-ATTRACTION', 'B-HOTEL', 'B-EATERY',\n",
    "                     'B-DRINKPLACE', 'B-CAMPING']\n",
    "    \n",
    "    extracted_tokens = []\n",
    "    domain = None\n",
    "    extracting = False\n",
    "\n",
    "    for token, label in zip(tokens, labels):\n",
    "        if not extracting:\n",
    "            if label in target_labels:\n",
    "                extracted_tokens.append(token)\n",
    "                domain = label.replace('B-', '')  # Lấy domain\n",
    "                extracting = True\n",
    "        else:\n",
    "            if label.startswith('I-'):\n",
    "                extracted_tokens.append(token)\n",
    "            else:\n",
    "                break  # Gặp O hoặc B- khác thì dừng\n",
    "\n",
    "    if extracted_tokens:\n",
    "        text = ' '.join(extracted_tokens)\n",
    "        return {\"text\": text, \"domain\": domain}\n",
    "    else:\n",
    "        return {\"text\": None, \"domain\": None}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93156959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_prediction(pred_array, aspects, sentiment_map):\n",
    "    pred_array = np.array(pred_array).flatten() \n",
    "\n",
    "    result = {}\n",
    "    for aspect, sentiment_id in zip(aspects, pred_array):\n",
    "        if sentiment_id != 0: \n",
    "            result[aspect] = sentiment_map[sentiment_id]\n",
    "\n",
    "    return result\n",
    "\n",
    "# decode_prediction(ypred_single, aspects, sentiment_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e7325b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(output_csv_path, text, place_extracted, domain_extracted, aspect_result):\n",
    "\n",
    "    record_result = {\n",
    "        'text': text,\n",
    "        'place_extracted': place_extracted,\n",
    "        'domain_extracted': domain_extracted,\n",
    "        'aspect_result': json.dumps(aspect_result, ensure_ascii=False)\n",
    "    }\n",
    "\n",
    "    df_new = pd.DataFrame([record_result])\n",
    "\n",
    "    if not os.path.exists(output_csv_path):\n",
    "        df_new.to_csv(output_csv_path, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(output_csv_path, mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d8eb7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "890de6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def ner_absa_udf(text):\n",
    "    global ner_model, ner_tokenizer\n",
    "    global attractions_model, attractions_vectorizer\n",
    "    global hotels_model, hotels_vectorizer\n",
    "    global restaurants_model, restaurants_vectorizer\n",
    "    global drinkplaces_model, drinkplaces_vectorizer\n",
    "    global eateries_model, eateries_vectorizer\n",
    "    global rents_model, rents_vectorizer\n",
    "    global tours_model, tours_vectorizer\n",
    "    global campings_model, campings_vectorizer\n",
    "\n",
    "    # Load mô hình NER nếu chưa có\n",
    "    if 'ner_model' not in globals():\n",
    "        from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "        config = Config()\n",
    "        ner_model = AutoModelForTokenClassification.from_pretrained(config.MODEL_NER_PATH)\n",
    "        ner_tokenizer = AutoTokenizer.from_pretrained(config.MODEL_NER_PATH)\n",
    "\n",
    "    # Load từng mô hình ABSA nếu chưa có\n",
    "    def load_absa_model(path, global_name_prefix):\n",
    "        if f'{global_name_prefix}_model' not in globals():\n",
    "            import pickle\n",
    "            with open(path, 'rb') as f:\n",
    "                package = pickle.load(f)\n",
    "                globals()[f'{global_name_prefix}_model'] = package['model']\n",
    "                globals()[f'{global_name_prefix}_vectorizer'] = package['vectorizer']\n",
    "\n",
    "    config = Config()\n",
    "    load_absa_model(config.ATTRACTIONS_ABSA_MODEL, 'attractions')\n",
    "    load_absa_model(config.HOTELS_ABSA_MODEL, 'hotels')\n",
    "    load_absa_model(config.RESTAURANTS_ABSA_MODEL, 'restaurants')\n",
    "    load_absa_model(config.DRINKPLACES_ABSA_MODEL, 'drinkplaces')\n",
    "    load_absa_model(config.EATERIES_ABSA_MODEL, 'eateries')\n",
    "    load_absa_model(config.RENTS_ABSA_MODEL, 'rents')\n",
    "    load_absa_model(config.TOURS_ABSA_MODEL, 'tours')\n",
    "    load_absa_model(config.CAMPINGS_ABSA_MODEL, 'campings')\n",
    "\n",
    "    # Gọi mô hình dự đoán\n",
    "    predict_tokens, predict_labels = predict_ner(text, ner_model, ner_tokenizer, id2label)\n",
    "    ner_result = extract_location_and_domain(predict_tokens, predict_labels)\n",
    "    place_extracted = ner_result['text']\n",
    "    domain_extracted = ner_result['domain']\n",
    "\n",
    "    # Xử lý domain và gọi mô hình tương ứng\n",
    "    domain_mapping = {\n",
    "        'TOUR': 'tours',\n",
    "        'RENT': 'rents',\n",
    "        'RESTAURANT': 'restaurants',\n",
    "        'ATTRACTION': 'attractions',\n",
    "        'HOTEL': 'hotels',\n",
    "        'EATERY': 'eateries',\n",
    "        'DRINKPLACE': 'drinkplaces',\n",
    "        'CAMPING': 'campings'\n",
    "    }\n",
    "\n",
    "    if domain_extracted in domain_mapping:\n",
    "        model = globals()[f\"{domain_mapping[domain_extracted]}_model\"]\n",
    "        vectorizer = globals()[f\"{domain_mapping[domain_extracted]}_vectorizer\"]\n",
    "        vector = vectorizer.transform([text])\n",
    "        prediction = model.predict(vector)\n",
    "        aspect_result = decode_prediction(prediction, eval(f\"{domain_mapping[domain_extracted]}_aspects\"), sentiment_map)\n",
    "    else:\n",
    "        place_extracted = ''\n",
    "        domain_extracted = ''\n",
    "        aspect_result = {}\n",
    "\n",
    "    return json.dumps({\n",
    "        \"place\": place_extracted,\n",
    "        \"domain\": domain_extracted,\n",
    "        \"aspect_result\": aspect_result\n",
    "    }, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45f89ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đăng ký UDF\n",
    "ner_absa_predict_udf = udf(ner_absa_udf, StringType())\n",
    "\n",
    "# Đọc dữ liệu từ Kafka\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \",\".join(config.KAFKA_SERVERS)) \\\n",
    "    .option(\"subscribe\", config.KAFKA_TOPIC_COMMENTS) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a6de947",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = df.selectExpr(\"CAST(value AS STRING) as json_str\")\n",
    "df_parsed = df_text.selectExpr(\"from_json(json_str, 'text STRING') as data\").select(\"data.text\")\n",
    "\n",
    "# Dự đoán hàng loạt (batch)\n",
    "df_with_prediction = df_parsed.withColumn(\"ner_absa_result\", ner_absa_predict_udf(col(\"text\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5267e299",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = df_with_prediction.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"path\", r\"F:\\Studies\\Third_year\\Big_data\\Final_Code\\Result\") \\\n",
    "    .option(\"checkpointLocation\", r\"F:\\Studies\\Third_year\\Big_data\\Final_Code\\Checkpoint\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a5b7ea74",
   "metadata": {},
   "outputs": [
    {
     "ename": "StreamingQueryException",
     "evalue": "[STREAM_FAILED] Query [id = 3648b729-3d18-4d5c-b303-be28cf8aca76, runId = 7f3809c9-7543-4869-972a-ee41f92a4aea] terminated with exception: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3) (LAPTOP-5DQQ5LDG.lan executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to F:\\Studies\\Third_year\\Big_data\\Final_Code\\Result.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:775)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$2(FileFormatWriter.scala:252)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"F:\\software\\spark-3.5.5-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1231, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\software\\spark-3.5.5-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1067, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\software\\spark-3.5.5-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 529, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\software\\spark-3.5.5-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 90, in read_command\n    command = serializer._read_with_length(file)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\software\\spark-3.5.5-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 174, in _read_with_length\n    return self.loads(obj)\n           ^^^^^^^^^^^^^^^\n  File \"F:\\software\\spark-3.5.5-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 472, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nModuleNotFoundError: No module named 'final_src'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\r\n\t... 12 more\r\n\nDriver stacktrace:",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mStreamingQueryException\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mquery\u001b[49m\u001b[43m.\u001b[49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mF:\\software\\spark-3.5.5-bin-hadoop3\\python\\pyspark\\sql\\streaming\\query.py:221\u001b[39m, in \u001b[36mStreamingQuery.awaitTermination\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    219\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jsq.awaitTermination(\u001b[38;5;28mint\u001b[39m(timeout * \u001b[32m1000\u001b[39m))\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jsq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mF:\\software\\spark-3.5.5-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mF:\\software\\spark-3.5.5-bin-hadoop3\\python\\pyspark\\errors\\exceptions\\captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mStreamingQueryException\u001b[39m: [STREAM_FAILED] Query [id = 3648b729-3d18-4d5c-b303-be28cf8aca76, runId = 7f3809c9-7543-4869-972a-ee41f92a4aea] terminated with exception: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3) (LAPTOP-5DQQ5LDG.lan executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to F:\\Studies\\Third_year\\Big_data\\Final_Code\\Result.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:775)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$2(FileFormatWriter.scala:252)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"F:\\software\\spark-3.5.5-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1231, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\software\\spark-3.5.5-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1067, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\software\\spark-3.5.5-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 529, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\software\\spark-3.5.5-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 90, in read_command\n    command = serializer._read_with_length(file)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\software\\spark-3.5.5-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 174, in _read_with_length\n    return self.loads(obj)\n           ^^^^^^^^^^^^^^^\n  File \"F:\\software\\spark-3.5.5-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 472, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nModuleNotFoundError: No module named 'final_src'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\r\n\t... 12 more\r\n\nDriver stacktrace:"
     ]
    }
   ],
   "source": [
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c460dfe5",
   "metadata": {},
   "outputs": [
    {
     "ename": "StreamingQueryException",
     "evalue": "[STREAM_FAILED] Query [id = 3648b729-3d18-4d5c-b303-be28cf8aca76, runId = 9c0c6f82-d001-4f82-ad7e-59009b820816] terminated with exception: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (LAPTOP-5DQQ5LDG.lan executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to F:\\Studies\\Third_year\\Big_data\\Final_Code\\Result.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:775)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$2(FileFormatWriter.scala:252)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"F:\\software\\spark-3.5.5-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1231, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\software\\spark-3.5.5-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1067, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\software\\spark-3.5.5-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 529, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\software\\spark-3.5.5-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 90, in read_command\n    command = serializer._read_with_length(file)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\software\\spark-3.5.5-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 174, in _read_with_length\n    return self.loads(obj)\n           ^^^^^^^^^^^^^^^\n  File \"F:\\software\\spark-3.5.5-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 472, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nModuleNotFoundError: No module named 'final_src'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\r\n\t... 12 more\r\n\nDriver stacktrace:",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mStreamingQueryException\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Ghi kết quả ra CSV\u001b[39;00m\n\u001b[32m     20\u001b[39m query = df_with_prediction.writeStream \\\n\u001b[32m     21\u001b[39m     .outputMode(\u001b[33m\"\u001b[39m\u001b[33mappend\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m     22\u001b[39m     .format(\u001b[33m\"\u001b[39m\u001b[33mcsv\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m     23\u001b[39m     .option(\u001b[33m\"\u001b[39m\u001b[33mpath\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mF:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mStudies\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mThird_year\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mBig_data\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mFinal_Code\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mResult\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m     24\u001b[39m     .option(\u001b[33m\"\u001b[39m\u001b[33mcheckpointLocation\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mF:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mStudies\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mThird_year\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mBig_data\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mFinal_Code\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mCheckpoint\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m     25\u001b[39m     .start()\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43mquery\u001b[49m\u001b[43m.\u001b[49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mF:\\software\\spark-3.5.5-bin-hadoop3\\python\\pyspark\\sql\\streaming\\query.py:221\u001b[39m, in \u001b[36mStreamingQuery.awaitTermination\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    219\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jsq.awaitTermination(\u001b[38;5;28mint\u001b[39m(timeout * \u001b[32m1000\u001b[39m))\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jsq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mF:\\software\\spark-3.5.5-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mF:\\software\\spark-3.5.5-bin-hadoop3\\python\\pyspark\\errors\\exceptions\\captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mStreamingQueryException\u001b[39m: [STREAM_FAILED] Query [id = 3648b729-3d18-4d5c-b303-be28cf8aca76, runId = 9c0c6f82-d001-4f82-ad7e-59009b820816] terminated with exception: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (LAPTOP-5DQQ5LDG.lan executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to F:\\Studies\\Third_year\\Big_data\\Final_Code\\Result.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:775)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$2(FileFormatWriter.scala:252)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"F:\\software\\spark-3.5.5-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1231, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\software\\spark-3.5.5-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1067, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\software\\spark-3.5.5-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 529, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\software\\spark-3.5.5-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 90, in read_command\n    command = serializer._read_with_length(file)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\software\\spark-3.5.5-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 174, in _read_with_length\n    return self.loads(obj)\n           ^^^^^^^^^^^^^^^\n  File \"F:\\software\\spark-3.5.5-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 472, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nModuleNotFoundError: No module named 'final_src'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\r\n\t... 12 more\r\n\nDriver stacktrace:"
     ]
    }
   ],
   "source": [
    "# Đăng ký UDF\n",
    "ner_absa_predict_udf = udf(ner_absa_udf, StringType())\n",
    "\n",
    "# Đọc dữ liệu từ Kafka\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \",\".join(config.KAFKA_SERVERS)) \\\n",
    "    .option(\"subscribe\", config.KAFKA_TOPIC_COMMENTS) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Giải mã và chọn trường text\n",
    "df_text = df.selectExpr(\"CAST(value AS STRING) as json_str\")\n",
    "df_parsed = df_text.selectExpr(\"from_json(json_str, 'text STRING') as data\").select(\"data.text\")\n",
    "\n",
    "# Dự đoán hàng loạt (batch)\n",
    "df_with_prediction = df_parsed.withColumn(\"ner_absa_result\", ner_absa_predict_udf(col(\"text\")))\n",
    "\n",
    "# Ghi kết quả ra CSV\n",
    "query = df_with_prediction.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"path\", r\"F:\\Studies\\Third_year\\Big_data\\Final_Code\\Result\") \\\n",
    "    .option(\"checkpointLocation\", r\"F:\\Studies\\Third_year\\Big_data\\Final_Code\\Checkpoint\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010b6cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prj__env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
