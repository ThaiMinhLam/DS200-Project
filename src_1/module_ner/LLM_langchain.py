import os
import google.generativeai as genai
import json
import pandas as pd
import re
import requests
from io import StringIO
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).resolve().parents[2]))
from src.config import Config
os.environ["GOOGLE_API_KEY"] = Config.GOOGLE_API_KEY
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain_google_genai import ChatGoogleGenerativeAI

class VietnameseNormalizer:
    """
    Tham kháº£o: https://github.com/VinAIResearch/BARTpho/blob/main/VietnameseToneNormalization.md
    """
    VINAI_NORMALIZED_TONE = {
        'Ã²a': 'oÃ ', 'Ã’a': 'OÃ ', 'Ã’A': 'OÃ€', 
        'Ã³a': 'oÃ¡', 'Ã“a': 'OÃ¡', 'Ã“A': 'OÃ', 
        'á»a': 'oáº£', 'á»a': 'Oáº£', 'á»A': 'Oáº¢',
        'Ãµa': 'oÃ£', 'Ã•a': 'OÃ£', 'Ã•A': 'OÃƒ',
        'á»a': 'oáº¡', 'á»Œa': 'Oáº¡', 'á»ŒA': 'Oáº ',
        'Ã²e': 'oÃ¨', 'Ã’e': 'OÃ¨', 'Ã’E': 'OÃˆ',
        'Ã³e': 'oÃ©', 'Ã“e': 'OÃ©', 'Ã“E': 'OÃ‰',
        'á»e': 'oáº»', 'á»e': 'Oáº»', 'á»E': 'Oáºº',
        'Ãµe': 'oáº½', 'Ã•e': 'Oáº½', 'Ã•E': 'Oáº¼',
        'á»e': 'oáº¹', 'á»Œe': 'Oáº¹', 'á»ŒE': 'Oáº¸',
        'Ã¹y': 'uá»³', 'Ã™y': 'Uá»³', 'Ã™Y': 'Uá»²',
        'Ãºy': 'uÃ½', 'Ãšy': 'UÃ½', 'ÃšY': 'UÃ',
        'á»§y': 'uá»·', 'á»¦y': 'Uá»·', 'á»¦Y': 'Uá»¶',
        'Å©y': 'uá»¹', 'Å¨y': 'Uá»¹', 'Å¨Y': 'Uá»¸',
        'á»¥y': 'uá»µ', 'á»¤y': 'Uá»µ', 'á»¤Y': 'Uá»´',
    }
    
    @staticmethod
    def normalize_unicode(text):
        char1252 = r'aÌ€|aÌ|aÌ‰|aÌƒ|aÌ£|Ã¢Ì€|Ã¢Ì|Ã¢Ì‰|Ã¢Ìƒ|Ã¢Ì£|ÄƒÌ€|ÄƒÌ|ÄƒÌ‰|ÄƒÌƒ|ÄƒÌ£|eÌ€|eÌ|eÌ‰|eÌƒ|eÌ£|ÃªÌ€|ÃªÌ|ÃªÌ‰|ÃªÌƒ|ÃªÌ£|iÌ€|iÌ|iÌ‰|iÌƒ|iÌ£|oÌ€|oÌ|oÌ‰|oÌƒ|oÌ£|Ã´Ì€|Ã´Ì|Ã´Ì‰|Ã´Ìƒ|Ã´Ì£|Æ¡Ì€|Æ¡Ì|Æ¡Ì‰|Æ¡Ìƒ|Æ¡Ì£|uÌ€|uÌ|uÌ‰|uÌƒ|uÌ£|Æ°Ì€|Æ°Ì|Æ°Ì‰|Æ°Ìƒ|Æ°Ì£|yÌ€|yÌ|yÌ‰|yÌƒ|yÌ£|AÌ€|AÌ|AÌ‰|AÌƒ|AÌ£|Ã‚Ì€|Ã‚Ì|Ã‚Ì‰|Ã‚Ìƒ|Ã‚Ì£|Ä‚Ì€|Ä‚Ì|Ä‚Ì‰|Ä‚Ìƒ|Ä‚Ì£|EÌ€|EÌ|EÌ‰|EÌƒ|EÌ£|ÃŠÌ€|ÃŠÌ|ÃŠÌ‰|ÃŠÌƒ|ÃŠÌ£|IÌ€|IÌ|IÌ‰|IÌƒ|IÌ£|OÌ€|OÌ|OÌ‰|OÌƒ|OÌ£|Ã”Ì€|Ã”Ì|Ã”Ì‰|Ã”Ìƒ|Ã”Ì£|Æ Ì€|Æ Ì|Æ Ì‰|Æ Ìƒ|Æ Ì£|UÌ€|UÌ|UÌ‰|UÌƒ|UÌ£|Æ¯Ì€|Æ¯Ì|Æ¯Ì‰|Æ¯Ìƒ|Æ¯Ì£|YÌ€|YÌ|YÌ‰|YÌƒ|YÌ£'
        charutf8 = r'Ã |Ã¡|áº£|Ã£|áº¡|áº§|áº¥|áº©|áº«|áº­|áº±|áº¯|áº³|áºµ|áº·|Ã¨|Ã©|áº»|áº½|áº¹|á»|áº¿|á»ƒ|á»…|á»‡|Ã¬|Ã­|á»‰|Ä©|á»‹|Ã²|Ã³|á»|Ãµ|á»|á»“|á»‘|á»•|á»—|á»™|á»|á»›|á»Ÿ|á»¡|á»£|Ã¹|Ãº|á»§|Å©|á»¥|á»«|á»©|á»­|á»¯|á»±|á»³|Ã½|á»·|á»¹|á»µ|Ã€|Ã|áº¢|Ãƒ|áº |áº¦|áº¤|áº¨|áºª|áº¬|áº°|áº®|áº²|áº´|áº¶|Ãˆ|Ã‰|áºº|áº¼|áº¸|á»€|áº¾|á»‚|á»„|á»†|ÃŒ|Ã|á»ˆ|Ä¨|á»Š|Ã’|Ã“|á»|Ã•|á»Œ|á»’|á»|á»”|á»–|á»˜|á»œ|á»š|á»|á» |á»¢|Ã™|Ãš|á»¦|Å¨|á»¤|á»ª|á»¨|á»¬|á»®|á»°|á»²|Ã|á»¶|á»¸|á»´'
        char_map = dict(zip(char1252.split('|'), charutf8.split('|')))
        return re.sub(char1252, lambda x: char_map[x.group()], text.strip())

    @staticmethod
    def normalize_typing(text):
        for wrong_word, correct_word in VietnameseNormalizer.VINAI_NORMALIZED_TONE.items():
            text = text.replace(wrong_word, correct_word)
        return text.strip()

class VietnameseCleaner:
    def remove_emoji(text):
        emoji_pattern = re.compile("["
            u"\U0001F600-\U0001F64F"  # emoticons
            u"\U0001F300-\U0001F5FF"  # symbols & pictographs
            u"\U0001F680-\U0001F6FF"  # transport & map symbols
            u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
            u"\U00002500-\U00002BEF"  # chinese char
            u"\U00002702-\U000027B0"
            u"\U000024C2-\U0001F251"
            u"\U0001f926-\U0001f937"
            u"\U00010000-\U0010ffff"
            u"\U0001F600-\U0001F64F"
            u"\U0001F300-\U0001F5FF"
            u"\U0001F680-\U0001F6FF"
            u"\U0001F1E0-\U0001F1FF"
            u"\U00002700-\U000027BF"
            u"\U000024C2-\U0001F251"            
                            "]+", flags=re.UNICODE)
        return emoji_pattern.sub(r'', text)

    def remove_punctuation_emoji(text):
        # XÃ³a cÃ¡c emoji: :))), :(((, =)))), =(((, ...
        text = re.sub(r'[:;=xX8@â‚«&]+-?[)(DPpOo3v]+', '', text)
        text = re.sub(r'[)(DPpOo3v]+-?[:;=xX8@â‚«&]+', '', text)
        text = re.sub(r'[:;=xX8@â‚«&]+[)(DPpOo3v]+', '', text)
        
        # XÃ³a cÃ¡c emoji @@, @.@, =.=, ...
        text = re.sub(r'[@=^~*]([.o_-])?[@=^~*]', '', text)
        
        # XÃ³a ngoáº·c thá»«a
        text = re.sub(r'\(\)', '', text)
        return text.strip()

    def remove_uncharacter_Vietnamese(text):
        ALLOWED_PUNCTUATION = r'\.,!?â€“:;'
        VN_CHARS = 'Ã¡Ã áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã©Ã¨áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã³Ã²á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£Ã­Ã¬á»‰Ä©á»‹ÃºÃ¹á»§Å©á»¥Æ°á»©á»«á»­á»¯á»±Ã½á»³á»·á»¹á»µÄ‘ÃÃ€áº¢Ãƒáº Ä‚áº®áº°áº²áº´áº¶Ã‚áº¤áº¦áº¨áºªáº¬Ã‰Ãˆáººáº¼áº¸ÃŠáº¾á»€á»‚á»„á»†Ã“Ã’á»Ã•á»ŒÃ”á»á»’á»”á»–á»˜Æ á»šá»œá»á» á»¢ÃÃŒá»ˆÄ¨á»ŠÃšÃ™á»¦Å¨á»¤Æ¯á»¨á»ªá»¬á»®á»°Ãá»²á»¶á»¸á»´Ä'
        text = re.sub(fr'[^\sa-zA-Z0-9{VN_CHARS}{ALLOWED_PUNCTUATION}]', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        return text

    @staticmethod
    def clean_social_text(text):
        text = VietnameseCleaner.remove_emoji(text)
        text = VietnameseCleaner.remove_punctuation_emoji(text)
        
        # remove html
        text = re.sub(r'<[^>]*>', '', text)
        
        # remove hashtag
        text = re.sub(r'#\w+', '', text)
        
        # remove url
        text = re.sub(r'https?://\S+|www\.\S+', '', text)
        
        # remove hotline
        text = re.sub(r'\b[(]?(\+84|0)[)]?\d{3}[-\s\.]?\d{3}[-\s\.]?\d{3,6}\b', '', text)
        
        # remove email
        text = re.sub(r'[^@ \t\r\n]+@[^@ \t\r\n]+\.[^@ \t\r\n]+', '', text)
        
        # remove repeated characters (giáº£m bá»›t cÆ°á»ng Ä‘á»™ cá»§a tá»«)
        text = re.sub(r'(.)\1{2,}', r'\1\1', text)

        # remove uncharacter + extra whitespace
        text = VietnameseCleaner.remove_uncharacter_Vietnamese(text)
        return text


class VietnameseTextProcessor:
    def __init__(self, max_correction_length=512):
        self.max_correction_length = max_correction_length
        self._build_teencodes()
    
    def _build_teencodes(self):
        self.teencodes = {
            'ok': ['okie', 'okey', 'Ã´kÃª', 'oki', 'oke', 'okay', 'okÃª'], 
            'khÃ´ng': ['kg', 'not', 'k', 'kh', 'kÃ´', 'hok', 'ko', 'khong'], 'khÃ´ng pháº£i': ['kp'], 
            'cáº£m Æ¡n': ['tks', 'thks', 'thanks', 'ths', 'thank'], 'há»“i Ä‘Ã³': ['hÃ¹i Ä‘Ã³'], 'muá»‘n': ['mÃºn'],
            
            'ráº¥t tá»‘t': ['perfect', 'â¤ï¸', 'ğŸ˜'], 'dá»… thÆ°Æ¡ng': ['cute'], 'yÃªu': ['iu'], 'thÃ­ch': ['thik'], 
            'tá»‘t': [
                'gud', 'good', 'gÃºt', 'tot', 'nice',
                'hehe', 'hihi', 'haha', 'hjhj', 'thick', '^_^', ':)', '=)'
                'ğŸ‘', 'ğŸ‰', 'ğŸ˜€', 'ğŸ˜‚', 'ğŸ¤—', 'ğŸ˜™', 'ğŸ™‚'
            ], 
            'bÃ¬nh thÆ°á»ng': ['bt', 'bthg'], 'hÃ g': ['hÃ ng'], 
            'khÃ´ng tá»‘t':  ['lol', 'cc', 'huhu', ':(', 'ğŸ˜”', 'ğŸ˜“'],
            'tá»‡': ['sad', 'por', 'poor', 'bad'], 'giáº£ máº¡o': ['fake'], 
            
            'quÃ¡': ['wa', 'wÃ¡', 'qÃ¡'], 'Ä‘Æ°á»£c': ['Ä‘x', 'dk', 'dc', 'Ä‘k', 'Ä‘c'], 
            'vá»›i': ['vs'], 'gÃ¬': ['j'], 'rá»“i': ['r'], 'mÃ¬nh': ['m', 'mik'], 
            'thá»i gian': ['time'], 'giá»': ['h'], 
        }
                
        self.teencodes = {word: key for key, values in self.teencodes.items() for word in values}
        teencode_url = 'https://gist.githubusercontent.com/behitek/7d9441c10b3c2739499fc5a4d9ea06fb/raw/df939245b3e841b62af115be4dcb3516dadc9fc5/teencode.txt'
        response = requests.get(teencode_url)
        
        if response.status_code == 200:
            text_data = StringIO(response.text)
            for pair in text_data:
                teencode, true_text = pair.split('\t')
                self.teencodes[teencode.strip()] = true_text.strip()
            self.teencodes = {k: self.teencodes[k] for k in sorted(self.teencodes)}
        else: print('Failed to fetch teencode.txt from', teencode_url)
        
    def correct_vietnamese_errors(self, texts):
        # https://huggingface.co/bmd1905/vietnamese-correction
        predictions = self.corrector(texts, max_length=self.max_correction_length, truncation=True)
        return [prediction['generated_text'] for prediction in predictions]
    
    def normalize_teencodes(self, text):
        words = []
        for word in text.split():
            words.append(self.teencodes.get(word, word))
        return ' '.join(words)
    
    def process_text(self, text, normalize_tone=True):
        # text = text.lower()
        if normalize_tone:
            text = VietnameseNormalizer.normalize_unicode(text)
            text = VietnameseNormalizer.normalize_typing(text)
        text = VietnameseCleaner.clean_social_text(text)
        text = self.normalize_teencodes(text)
        return text
    
class NERExtractor:
    def __init__(self, llm):
        self.prompt_template = PromptTemplate(
            input_variables=["text"],
            template="""
Báº¡n lÃ  cÃ´ng cá»¥ NER, nhiá»‡m vá»¥ cá»§a báº¡n lÃ  **trÃ­ch xuáº¥t cÃ¡c cá»¥m danh tá»« tiáº¿ng Viá»‡t liÃªn quan Ä‘áº¿n Ä‘á»‹a Ä‘iá»ƒm vÃ  dá»‹ch vá»¥ du lá»‹ch á»Ÿ ThÃ nh phá»‘ ÄÃ  Láº¡t, Viá»‡t Nam** trong Ä‘oáº¡n vÄƒn dÆ°á»›i Ä‘Ã¢y. Chá»‰ trÃ­ch xuáº¥t **cá»¥m tá»« Ä‘áº§y Ä‘á»§**, khÃ´ng bá» sÃ³t tá»« mÃ´ táº£ Ä‘i kÃ¨m, khÃ´ng trÃ­ch rá»i láº».
Äoáº¡n vÄƒn:
{text}

### Äá»‹nh nghÄ©a cÃ¡c nhÃ³m:
- **HOTELS**: KhÃ¡ch sáº¡n, homestay, villa, resort, nhÃ  nghá»‰, nÆ¡i lÆ°u trÃº.
- **RESTAURANTS**: QuÃ¡n Äƒn, nhÃ  hÃ ng, tiá»‡m cÆ¡m, quÃ¡n nháº­u.
- **DRINKPLACES**: QuÃ¡n cÃ  phÃª, quÃ¡n trÃ , quÃ¡n nÆ°á»›c, quÃ¡n bar, quÃ¡n pub.
- **STREETFOODRESTAURANT**: Xe Ä‘áº©y, gÃ¡nh hÃ ng rong, tiá»‡m bÃ¡nh cÄƒn, quÃ¡n vá»‰a hÃ¨.
- **ATTRACTIONS**: Äá»‹a Ä‘iá»ƒm tham quan, Ä‘iá»ƒm check-in, cÃ´ng viÃªn, Ä‘á»“i, thÃ¡c, há»“, khu du lá»‹ch.
- **RENTALSERVICES**: Dá»‹ch vá»¥ cho thuÃª xe mÃ¡y, xe Ä‘áº¡p, Ã´ tÃ´, lá»u tráº¡i.
- **TOURS**: Tour du lá»‹ch, tour trekking, tour tráº£i nghiá»‡m, tour tham quan.
- **CAMPINGS**: Äá»‹a Ä‘iá»ƒm cáº¯m tráº¡i.

### Quy Ä‘á»‹nh output:
- Chá»‰ tráº£ káº¿t quáº£ **dáº¡ng JSON array**, má»—i pháº§n tá»­ gá»“m:
```json
{{
    "text": "cá»¥m tá»« Ä‘Æ°á»£c trÃ­ch",
    "label": "tÃªn nhÃ³m"
}}
- Náº¿u khÃ´ng cÃ³ pháº§n tá»­ thÃ¬ tráº£ vá» rá»—ng
                """
            )
        self.chain = LLMChain(llm=llm, prompt=self.prompt_template)

    def extract(self, text: str, parse_json: bool = True):
        """
        text: cÃ¢u tiáº¿ng Viá»‡t cáº§n detect NER
        parse_json: náº¿u True, tá»± parse JSON tráº£ vá», náº¿u lá»—i tráº£ raw string
        """
        result = self.chain.run(text=text)
        cleaned_result = clean_result(result)
        # print(cleaned_result)
        return cleaned_result
    
@staticmethod
def clean_result(result):
    result = result.strip()
    if result.startswith("```"):
        # Loáº¡i bá» dÃ²ng Ä‘áº§u ```json hoáº·c ```
        lines = result.split("\n")
        if lines[0].startswith("```"):
            lines = lines[1:]
        # Loáº¡i bá» dÃ²ng cuá»‘i ```
        if lines[-1].startswith("```"):
            lines = lines[:-1]
        result = "\n".join(lines).strip()
    return result

def ner_extract(text):
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest", temperature=0)
    ner_extractor = NERExtractor(llm)
    result = ner_extractor.extract(text)
    return(json.loads(result))
    
if __name__ == "__main__":
    ner_extract()
