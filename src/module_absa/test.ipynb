{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e6499c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from io import StringIO\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyCIFghSOEJMA8kPzIP7n40OyCwFYGVaanc\"\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e28fcb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VietnameseNormalizer:\n",
    "    \"\"\"\n",
    "    Tham kh·∫£o: https://github.com/VinAIResearch/BARTpho/blob/main/VietnameseToneNormalization.md\n",
    "    \"\"\"\n",
    "    VINAI_NORMALIZED_TONE = {\n",
    "        '√≤a': 'o√†', '√ía': 'O√†', '√íA': 'O√Ä', \n",
    "        '√≥a': 'o√°', '√ìa': 'O√°', '√ìA': 'O√Å', \n",
    "        '·ªèa': 'o·∫£', '·ªéa': 'O·∫£', '·ªéA': 'O·∫¢',\n",
    "        '√µa': 'o√£', '√ïa': 'O√£', '√ïA': 'O√É',\n",
    "        '·ªça': 'o·∫°', '·ªåa': 'O·∫°', '·ªåA': 'O·∫†',\n",
    "        '√≤e': 'o√®', '√íe': 'O√®', '√íE': 'O√à',\n",
    "        '√≥e': 'o√©', '√ìe': 'O√©', '√ìE': 'O√â',\n",
    "        '·ªèe': 'o·∫ª', '·ªée': 'O·∫ª', '·ªéE': 'O·∫∫',\n",
    "        '√µe': 'o·∫Ω', '√ïe': 'O·∫Ω', '√ïE': 'O·∫º',\n",
    "        '·ªçe': 'o·∫π', '·ªåe': 'O·∫π', '·ªåE': 'O·∫∏',\n",
    "        '√πy': 'u·ª≥', '√ôy': 'U·ª≥', '√ôY': 'U·ª≤',\n",
    "        '√∫y': 'u√Ω', '√öy': 'U√Ω', '√öY': 'U√ù',\n",
    "        '·ªßy': 'u·ª∑', '·ª¶y': 'U·ª∑', '·ª¶Y': 'U·ª∂',\n",
    "        '≈©y': 'u·ªπ', '≈®y': 'U·ªπ', '≈®Y': 'U·ª∏',\n",
    "        '·ª•y': 'u·ªµ', '·ª§y': 'U·ªµ', '·ª§Y': 'U·ª¥',\n",
    "    }\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_unicode(text):\n",
    "        char1252 = r'aÃÄ|aÃÅ|aÃâ|aÃÉ|aÃ£|√¢ÃÄ|√¢ÃÅ|√¢Ãâ|√¢ÃÉ|√¢Ã£|ƒÉÃÄ|ƒÉÃÅ|ƒÉÃâ|ƒÉÃÉ|ƒÉÃ£|eÃÄ|eÃÅ|eÃâ|eÃÉ|eÃ£|√™ÃÄ|√™ÃÅ|√™Ãâ|√™ÃÉ|√™Ã£|iÃÄ|iÃÅ|iÃâ|iÃÉ|iÃ£|oÃÄ|oÃÅ|oÃâ|oÃÉ|oÃ£|√¥ÃÄ|√¥ÃÅ|√¥Ãâ|√¥ÃÉ|√¥Ã£|∆°ÃÄ|∆°ÃÅ|∆°Ãâ|∆°ÃÉ|∆°Ã£|uÃÄ|uÃÅ|uÃâ|uÃÉ|uÃ£|∆∞ÃÄ|∆∞ÃÅ|∆∞Ãâ|∆∞ÃÉ|∆∞Ã£|yÃÄ|yÃÅ|yÃâ|yÃÉ|yÃ£|AÃÄ|AÃÅ|AÃâ|AÃÉ|AÃ£|√ÇÃÄ|√ÇÃÅ|√ÇÃâ|√ÇÃÉ|√ÇÃ£|ƒÇÃÄ|ƒÇÃÅ|ƒÇÃâ|ƒÇÃÉ|ƒÇÃ£|EÃÄ|EÃÅ|EÃâ|EÃÉ|EÃ£|√äÃÄ|√äÃÅ|√äÃâ|√äÃÉ|√äÃ£|IÃÄ|IÃÅ|IÃâ|IÃÉ|IÃ£|OÃÄ|OÃÅ|OÃâ|OÃÉ|OÃ£|√îÃÄ|√îÃÅ|√îÃâ|√îÃÉ|√îÃ£|∆†ÃÄ|∆†ÃÅ|∆†Ãâ|∆†ÃÉ|∆†Ã£|UÃÄ|UÃÅ|UÃâ|UÃÉ|UÃ£|∆ØÃÄ|∆ØÃÅ|∆ØÃâ|∆ØÃÉ|∆ØÃ£|YÃÄ|YÃÅ|YÃâ|YÃÉ|YÃ£'\n",
    "        charutf8 = r'√†|√°|·∫£|√£|·∫°|·∫ß|·∫•|·∫©|·∫´|·∫≠|·∫±|·∫Ø|·∫≥|·∫µ|·∫∑|√®|√©|·∫ª|·∫Ω|·∫π|·ªÅ|·∫ø|·ªÉ|·ªÖ|·ªá|√¨|√≠|·ªâ|ƒ©|·ªã|√≤|√≥|·ªè|√µ|·ªç|·ªì|·ªë|·ªï|·ªó|·ªô|·ªù|·ªõ|·ªü|·ª°|·ª£|√π|√∫|·ªß|≈©|·ª•|·ª´|·ª©|·ª≠|·ªØ|·ª±|·ª≥|√Ω|·ª∑|·ªπ|·ªµ|√Ä|√Å|·∫¢|√É|·∫†|·∫¶|·∫§|·∫®|·∫™|·∫¨|·∫∞|·∫Æ|·∫≤|·∫¥|·∫∂|√à|√â|·∫∫|·∫º|·∫∏|·ªÄ|·∫æ|·ªÇ|·ªÑ|·ªÜ|√å|√ç|·ªà|ƒ®|·ªä|√í|√ì|·ªé|√ï|·ªå|·ªí|·ªê|·ªî|·ªñ|·ªò|·ªú|·ªö|·ªû|·ª†|·ª¢|√ô|√ö|·ª¶|≈®|·ª§|·ª™|·ª®|·ª¨|·ªÆ|·ª∞|·ª≤|√ù|·ª∂|·ª∏|·ª¥'\n",
    "        char_map = dict(zip(char1252.split('|'), charutf8.split('|')))\n",
    "        return re.sub(char1252, lambda x: char_map[x.group()], text.strip())\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_typing(text):\n",
    "        for wrong_word, correct_word in VietnameseNormalizer.VINAI_NORMALIZED_TONE.items():\n",
    "            text = text.replace(wrong_word, correct_word)\n",
    "        return text.strip()\n",
    "\n",
    "class VietnameseCleaner:\n",
    "    def remove_emoji(text):\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "            u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "            u\"\\U00002702-\\U000027B0\"\n",
    "            u\"\\U000024C2-\\U0001F251\"\n",
    "            u\"\\U0001f926-\\U0001f937\"\n",
    "            u\"\\U00010000-\\U0010ffff\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"\n",
    "            u\"\\U0001F300-\\U0001F5FF\"\n",
    "            u\"\\U0001F680-\\U0001F6FF\"\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "            u\"\\U00002700-\\U000027BF\"\n",
    "            u\"\\U000024C2-\\U0001F251\"            \n",
    "                            \"]+\", flags=re.UNICODE)\n",
    "        return emoji_pattern.sub(r'', text)\n",
    "\n",
    "    def remove_punctuation_emoji(text):\n",
    "        # X√≥a c√°c emoji: :))), :(((, =)))), =(((, ...\n",
    "        text = re.sub(r'[:;=xX8@‚Ç´&]+-?[)(DPpOo3v]+', '', text)\n",
    "        text = re.sub(r'[)(DPpOo3v]+-?[:;=xX8@‚Ç´&]+', '', text)\n",
    "        text = re.sub(r'[:;=xX8@‚Ç´&]+[)(DPpOo3v]+', '', text)\n",
    "        \n",
    "        # X√≥a c√°c emoji @@, @.@, =.=, ...\n",
    "        text = re.sub(r'[@=^~*]([.o_-])?[@=^~*]', '', text)\n",
    "        \n",
    "        # X√≥a ngo·∫∑c th·ª´a\n",
    "        text = re.sub(r'\\(\\)', '', text)\n",
    "        return text.strip()\n",
    "\n",
    "    def remove_uncharacter_Vietnamese(text):\n",
    "        ALLOWED_PUNCTUATION = r'\\.,!?‚Äì:;'\n",
    "        VN_CHARS = '√°√†·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√©√®·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√≥√≤·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√≠√¨·ªâƒ©·ªã√∫√π·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±√Ω·ª≥·ª∑·ªπ·ªµƒë√Å√Ä·∫¢√É·∫†ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨√â√à·∫∫·∫º·∫∏√ä·∫æ·ªÄ·ªÇ·ªÑ·ªÜ√ì√í·ªé√ï·ªå√î·ªê·ªí·ªî·ªñ·ªò∆†·ªö·ªú·ªû·ª†·ª¢√ç√å·ªàƒ®·ªä√ö√ô·ª¶≈®·ª§∆Ø·ª®·ª™·ª¨·ªÆ·ª∞√ù·ª≤·ª∂·ª∏·ª¥ƒê'\n",
    "        text = re.sub(fr'[^\\sa-zA-Z0-9{VN_CHARS}{ALLOWED_PUNCTUATION}]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_social_text(text):\n",
    "        text = VietnameseCleaner.remove_emoji(text)\n",
    "        text = VietnameseCleaner.remove_punctuation_emoji(text)\n",
    "        \n",
    "        # remove html\n",
    "        text = re.sub(r'<[^>]*>', '', text)\n",
    "        \n",
    "        # remove hashtag\n",
    "        text = re.sub(r'#\\w+', '', text)\n",
    "        \n",
    "        # remove url\n",
    "        text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "        \n",
    "        # remove hotline\n",
    "        text = re.sub(r'\\b[(]?(\\+84|0)[)]?\\d{3}[-\\s\\.]?\\d{3}[-\\s\\.]?\\d{3,6}\\b', '', text)\n",
    "        \n",
    "        # remove email\n",
    "        text = re.sub(r'[^@ \\t\\r\\n]+@[^@ \\t\\r\\n]+\\.[^@ \\t\\r\\n]+', '', text)\n",
    "        \n",
    "        # remove repeated characters (gi·∫£m b·ªõt c∆∞·ªùng ƒë·ªô c·ªßa t·ª´)\n",
    "        text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
    "\n",
    "        # remove uncharacter + extra whitespace\n",
    "        text = VietnameseCleaner.remove_uncharacter_Vietnamese(text)\n",
    "        return text\n",
    "\n",
    "\n",
    "class VietnameseTextProcessor:\n",
    "    def __init__(self, max_correction_length=512):\n",
    "        self.max_correction_length = max_correction_length\n",
    "        self._build_teencodes()\n",
    "    \n",
    "    def _build_teencodes(self):\n",
    "        self.teencodes = {\n",
    "            'ok': ['okie', 'okey', '√¥k√™', 'oki', 'oke', 'okay', 'ok√™'], \n",
    "            'kh√¥ng': ['kg', 'not', 'k', 'kh', 'k√¥', 'hok', 'ko', 'khong'], 'kh√¥ng ph·∫£i': ['kp'], \n",
    "            'c·∫£m ∆°n': ['tks', 'thks', 'thanks', 'ths', 'thank'], 'h·ªìi ƒë√≥': ['h√πi ƒë√≥'], 'mu·ªën': ['m√∫n'],\n",
    "            \n",
    "            'r·∫•t t·ªët': ['perfect', '‚ù§Ô∏è', 'üòç'], 'd·ªÖ th∆∞∆°ng': ['cute'], 'y√™u': ['iu'], 'th√≠ch': ['thik'], \n",
    "            't·ªët': [\n",
    "                'gud', 'good', 'g√∫t', 'tot', 'nice',\n",
    "                'hehe', 'hihi', 'haha', 'hjhj', 'thick', '^_^', ':)', '=)'\n",
    "                'üëç', 'üéâ', 'üòÄ', 'üòÇ', 'ü§ó', 'üòô', 'üôÇ'\n",
    "            ], \n",
    "            'b√¨nh th∆∞·ªùng': ['bt', 'bthg'], 'h√†g': ['h√†ng'], \n",
    "            'kh√¥ng t·ªët':  ['lol', 'cc', 'huhu', ':(', 'üòî', 'üòì'],\n",
    "            't·ªá': ['sad', 'por', 'poor', 'bad'], 'gi·∫£ m·∫°o': ['fake'], \n",
    "            \n",
    "            'qu√°': ['wa', 'w√°', 'q√°'], 'ƒë∆∞·ª£c': ['ƒëx', 'dk', 'dc', 'ƒëk', 'ƒëc'], \n",
    "            'v·ªõi': ['vs'], 'g√¨': ['j'], 'r·ªìi': ['r'], 'm√¨nh': ['m', 'mik'], \n",
    "            'th·ªùi gian': ['time'], 'gi·ªù': ['h'], \n",
    "        }\n",
    "                \n",
    "        self.teencodes = {word: key for key, values in self.teencodes.items() for word in values}\n",
    "        teencode_url = 'https://gist.githubusercontent.com/behitek/7d9441c10b3c2739499fc5a4d9ea06fb/raw/df939245b3e841b62af115be4dcb3516dadc9fc5/teencode.txt'\n",
    "        response = requests.get(teencode_url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            text_data = StringIO(response.text)\n",
    "            for pair in text_data:\n",
    "                teencode, true_text = pair.split('\\t')\n",
    "                self.teencodes[teencode.strip()] = true_text.strip()\n",
    "            self.teencodes = {k: self.teencodes[k] for k in sorted(self.teencodes)}\n",
    "        else: print('Failed to fetch teencode.txt from', teencode_url)\n",
    "        \n",
    "    def correct_vietnamese_errors(self, texts):\n",
    "        # https://huggingface.co/bmd1905/vietnamese-correction\n",
    "        predictions = self.corrector(texts, max_length=self.max_correction_length, truncation=True)\n",
    "        return [prediction['generated_text'] for prediction in predictions]\n",
    "    \n",
    "    def normalize_teencodes(self, text):\n",
    "        words = []\n",
    "        for word in text.split():\n",
    "            words.append(self.teencodes.get(word, word))\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    def process_text(self, text, normalize_tone=True):\n",
    "        # text = text.lower()\n",
    "        if normalize_tone:\n",
    "            text = VietnameseNormalizer.normalize_unicode(text)\n",
    "            text = VietnameseNormalizer.normalize_typing(text)\n",
    "        text = VietnameseCleaner.clean_social_text(text)\n",
    "        text = self.normalize_teencodes(text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "986a7f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aspect_dict cho ABSA\n",
    "ASPECT_DICT = {\n",
    "    \"HOTELS\": [\n",
    "        \"HOTEL#LOCATION\", \"HOTEL#QUALITY\", \"HOTEL#FACILITIES\", \"HOTEL#STYLE\",\n",
    "        \"WIFI\", \"PRICE\", \"ROOM#QUALITY\", \"ROOM#STYLE\", \"ROOM#FACILITIES\",\n",
    "        \"ROOM#SOUND\", \"ROOM#VIEW\", \"ROOM#ATMOSPHERE\", \"ROOM#CLEANLINESS\",\n",
    "        \"SERVICE#STAFF\", \"SERVICE#CHECKIN\"\n",
    "    ],\n",
    "    \"RESTAURANTS\": [\n",
    "        \"LOCATION\", \"PRICE\", \"FOOD#QUALITY\", \"FOOD#VARIETY\",\n",
    "        \"FOOD#PRESENTATION\", \"FOOD#FRESHNESS\", \"DRINK#QUALITY\",\n",
    "        \"ENVIRONMENT#CLEANLINESS\", \"ENVIRONMENT#AMBIENCE\",\n",
    "        \"SERVICE#STAFF\", \"SERVICE#ORDER\"\n",
    "    ],\n",
    "    \"DRINKPLACES\": [\n",
    "        \"LOCATION\", \"PRICE\", \"FOOD#QUALITY\", \"DRINK#QUALITY\",\n",
    "        \"DRINK#VARIETY\", \"ENVIRONMENT#CLEANLINESS\", \"ENVIRONMENT#AMBIENCE\",\n",
    "        \"SERVICE#STAFF\", \"SERVICE#ORDER\"\n",
    "    ],\n",
    "    \"EATERY\": [\n",
    "        \"LOCATION\", \"PRICE\", \"FOOD#QUALITY\", \"FOOD#VARIETY\",\n",
    "        \"DRINK#QUALITY\", \"DRINK#VARIETY\", \"ENVIRONMENT#CLEANLINESS\",\n",
    "        \"ENVIRONMENT#AMBIENCE\", \"SERVICE#STAFF\", \"SERVICE#ORDER\"\n",
    "    ],\n",
    "    \"ATTRACTIONS\": [\n",
    "        \"LOCATION\", \"PRICE\", \"SERVICE#STAFF\", \"SERVICE#BOOKING\",\n",
    "        \"ENVIRONMENT#SCENERY\", \"ENVIRONMENT#ATMOSPHERE\",\n",
    "        \"EXPERIENCE#ACTIVITY\"\n",
    "    ],\n",
    "    \"RENTALSERVICES\": [\n",
    "        \"LOCATION\", \"PRICE\", \"SERVICE#RENTING\", \"SERVICE#STAFF\",\n",
    "        \"VEHICLE#QUALITY\"\n",
    "    ],\n",
    "    \"TOUR\": [\n",
    "        \"LOCATION\", \"PRICE\", \"SERVICE#STAFF\", \"EXPERIENCE#ACTIVITY\",\n",
    "        \"ENVIRONMENT#SCENERY\", \"ENVIRONMENT#ATMOSPHERE\"\n",
    "    ],\n",
    "    \"CAMPING\": [\n",
    "        \"LOCATION#DISTANCE\", \"LOCATION#ACCESSIBILITY\", \"SERVICE#STAFF\",\n",
    "        \"ENVIRONMENT#SCENERY\", \"ENVIRONMENT#WEATHER\", \"ENVIRONMENT#ATMOSPHERE\"\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de729c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "def build_absa_prompt_template(domain: str) -> PromptTemplate:\n",
    "    # if domain not in ASPECT_DICT:\n",
    "    #     raise ValueError(f\"Domain '{domain}' kh√¥ng t·ªìn t·∫°i. Ch·ªçn t·ª´: {list(ASPECT_DICT.keys())}\")\n",
    "\n",
    "    # aspects = ASPECT_DICT[domain]\n",
    "    # aspect_list_str = \"\\n- \".join(aspects)\n",
    "\n",
    "    template_text = f\"\"\"\n",
    "B·∫°n l√† c√¥ng c·ª• g√°n nh√£n Aspect-Based Sentiment Analysis (ABSA) cho **review ti·∫øng Vi·ªát** v·ªÅ d·ªãch v·ª• thu·ªôc lƒ©nh v·ª±c **{domain}** t·∫°i Vi·ªát Nam.\n",
    "Nhi·ªám v·ª•:\n",
    "- X√°c ƒë·ªãnh c√°c aspect xu·∫•t hi·ªán trong review d∆∞·ªõi ƒë√¢y v√† g√°n sentiment ph√π h·ª£p.\n",
    "- Ch·ªâ s·ª≠ d·ª•ng **danh s√°ch aspect d∆∞·ªõi ƒë√¢y**, kh√¥ng t·ª± nghƒ© th√™m aspect kh√°c.\n",
    "- Sentiment ch·ªâ ƒë∆∞·ª£c g√°n l√†: \"POSITIVE\", \"NEUTRAL\", \"NEGATIVE\".\n",
    "\n",
    "### Danh s√°ch aspect:\n",
    "- {ASPECT_DICT}\n",
    "\n",
    "### Quy ƒë·ªãnh output:\n",
    "- Ch·ªâ tr·∫£ v·ªÅ **d·∫°ng list JSON**, m·ªói ph·∫ßn t·ª≠ g·ªìm:\n",
    "{{\n",
    "    \"aspect\": \"ASPECT_NAME\",\n",
    "    \"sentiment\": \"POSITIVE/NEUTRAL/NEGATIVE\"\n",
    "}}\n",
    "- N·∫øu kh√¥ng c√≥ aspect n√†o, tr·∫£ v·ªÅ: []\n",
    "\n",
    "### Review:\n",
    "{{text}}\n",
    "\"\"\"\n",
    "    return PromptTemplate(\n",
    "        input_variables=[\"text\"],\n",
    "        template=template_text\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ca57b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ABSAExtractor:\n",
    "    def __init__(self, llm):\n",
    "        self.prompt_template = PromptTemplate(\n",
    "            input_variables=[\"text\", \"domain\", \"aspects\"],\n",
    "            template=\"\"\"\n",
    "B·∫°n l√† c√¥ng c·ª• Aspect-Based Sentiment Analysis (ABSA) cho **review ti·∫øng Vi·ªát** v·ªÅ d·ªãch v·ª• trong lƒ©nh v·ª±c **{domain}** t·∫°i ƒê√† L·∫°t, Vi·ªát Nam.\n",
    "Nhi·ªám v·ª•:\n",
    "- X√°c ƒë·ªãnh c√°c aspect xu·∫•t hi·ªán trong review d∆∞·ªõi ƒë√¢y v√† g√°n sentiment ph√π h·ª£p.\n",
    "- Ch·ªâ s·ª≠ d·ª•ng **danh s√°ch aspect d∆∞·ªõi ƒë√¢y**, kh√¥ng t·ª± nghƒ© th√™m aspect kh√°c.\n",
    "- Sentiment ch·ªâ ƒë∆∞·ª£c g√°n: \"POSITIVE\", \"NEUTRAL\", \"NEGATIVE\".\n",
    "\n",
    "### Danh s√°ch aspect:\n",
    "{aspects}\n",
    "\n",
    "### Quy ƒë·ªãnh output:\n",
    "- Ch·ªâ tr·∫£ v·ªÅ **d·∫°ng list JSON**, m·ªói ph·∫ßn t·ª≠ g·ªìm:\n",
    "{{\"aspect\": \"ASPECT_NAME\", \"sentiment\": \"POSITIVE/NEUTRAL/NEGATIVE\"}}\n",
    "- N·∫øu kh√¥ng c√≥ aspect n√†o th√¨ tr·∫£ v·ªÅ: []\n",
    "\n",
    "### Review:\n",
    "{text}\n",
    "\"\"\"\n",
    "        )\n",
    "        self.chain = LLMChain(llm=llm, prompt=self.prompt_template)\n",
    "\n",
    "    def extract(self, text: str, domain: str, aspects_list: list, parse_json: bool = True):\n",
    "        \"\"\"\n",
    "        text: c√¢u review c·∫ßn g√°n ABSA\n",
    "        domain: t√™n domain (v√≠ d·ª•: HOTELS)\n",
    "        aspects_list: list c√°c aspect (s·∫Ω convert sang d·∫°ng string ph√π h·ª£p)\n",
    "        parse_json: n·∫øu True, t·ª± parse JSON tr·∫£ v·ªÅ, n·∫øu l·ªói th√¨ tr·∫£ raw string\n",
    "        \"\"\"\n",
    "        aspects_formatted = \"\\n\".join(f\"- {aspect}\" for aspect in aspects_list)\n",
    "\n",
    "        result = self.chain.run(\n",
    "            text=text,\n",
    "            domain=domain,\n",
    "            aspects=aspects_formatted\n",
    "        )\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "639d3200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "[\n",
      "  {\"aspect\": \"ROOM#CLEANLINESS\", \"sentiment\": \"POSITIVE\"},\n",
      "  {\"aspect\": \"ROOM#VIEW\", \"sentiment\": \"POSITIVE\"},\n",
      "  {\"aspect\": \"SERVICE#STAFF\", \"sentiment\": \"POSITIVE\"},\n",
      "  {\"aspect\": \"PRICE\", \"sentiment\": \"NEGATIVE\"}\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\", temperature=0)\n",
    "\n",
    "absa_extractor = ABSAExtractor(llm)\n",
    "\n",
    "text = \"Ph√≤ng s·∫°ch, view ƒë·∫πp, nh√¢n vi√™n vui v·∫ª, gi√° h∆°i cao.\"\n",
    "domain = \"HOTELS\"\n",
    "aspect_list = ASPECT_DICT[domain]\n",
    "result = absa_extractor.extract(text=text, domain=domain, aspects_list=aspect_list)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4c9e724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_result(result):\n",
    "    result = result.strip()\n",
    "    if result.startswith(\"```\"):\n",
    "        # Lo·∫°i b·ªè d√≤ng ƒë·∫ßu ```json ho·∫∑c ```\n",
    "        lines = result.split(\"\\n\")\n",
    "        if lines[0].startswith(\"```\"):\n",
    "            lines = lines[1:]\n",
    "        # Lo·∫°i b·ªè d√≤ng cu·ªëi ```\n",
    "        if lines[-1].startswith(\"```\"):\n",
    "            lines = lines[:-1]\n",
    "        result = \"\\n\".join(lines).strip()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "80b5563d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = clean_result(result)\n",
    "temp2 = json.loads(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d7057dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "35d7ba1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'aspect': 'ROOM#CLEANLINESS', 'sentiment': 'POSITIVE'},\n",
       " {'aspect': 'ROOM#VIEW', 'sentiment': 'POSITIVE'},\n",
       " {'aspect': 'SERVICE#STAFF', 'sentiment': 'POSITIVE'},\n",
       " {'aspect': 'PRICE', 'sentiment': 'NEGATIVE'}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a09d7e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
