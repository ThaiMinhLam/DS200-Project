{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c70c724",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\.conda\\envs\\prj__env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaConsumer, KafkaProducer\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "import pickle\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(r'F:\\Studies\\Third_year\\Big_data\\Final_Code')\n",
    "from final_src.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "011d22e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "drinkplaces_aspects = ['DRINK#QUALITY',\n",
    " 'DRINK#VARIETY',\n",
    " 'ENVIRONMENT#CLEANLINESS',\n",
    " 'ENVIRONMENT#AMBIENCE',\n",
    " 'FOOD#QUALITY',\n",
    " 'LOCATION',\n",
    " 'PRICE',\n",
    " 'SERVICE#ORDER',\n",
    " 'SERVICE#STAFF']\n",
    "\n",
    "hotels_aspects = ['HOTEL#LOCATION',\n",
    " 'HOTEL#QUALITY',\n",
    " 'HOTEL#FACILITIES',\n",
    " 'HOTEL#STYLE',\n",
    " 'WIFI',\n",
    " 'PRICE',\n",
    " 'ROOM#QUALITY',\n",
    " 'ROOM#STYLE',\n",
    " 'ROOM#FACILITIES',\n",
    " 'ROOM#SOUND',\n",
    " 'ROOM#VIEW',\n",
    " 'ROOM#ATMOSPHERE',\n",
    " 'ROOM#CLEANLINESS',\n",
    " 'SERVICE#STAFF',\n",
    " 'SERVICE#CHECKIN']\n",
    "\n",
    "restaurants_aspects = ['LOCATION',\n",
    " 'PRICE',\n",
    " 'FOOD#QUALITY',\n",
    " 'FOOD#VARIETY',\n",
    " 'FOOD#PRESENTATION',\n",
    " 'FOOD#FRESHNESS',\n",
    " 'DRINK#QUALITY',\n",
    " 'ENVIRONMENT#CLEANLINESS',\n",
    " 'ENVIRONMENT#AMBIENCE',\n",
    " 'SERVICE#STAFF',\n",
    " 'SERVICE#ORDER']\n",
    "\n",
    "eateries_aspects = ['LOCATION',\n",
    " 'PRICE',\n",
    " 'FOOD#QUALITY',\n",
    " 'FOOD#VARIETY',\n",
    " 'DRINK#QUALITY',\n",
    " 'DRINK#VARIETY',\n",
    " 'ENVIRONMENT#CLEANLINESS',\n",
    " 'ENVIRONMENT#AMBIENCE',\n",
    " 'SERVICE#STAFF',\n",
    " 'SERVICE#ORDER']\n",
    "\n",
    "attractions_aspects = ['LOCATION',\n",
    " 'PRICE',\n",
    " 'SERVICE#STAFF',\n",
    " 'ENVIRONMENT#SCENERY',\n",
    " 'ENVIRONMENT#ATMOSPHERE',\n",
    " 'EXPERIENCE#ACTIVITY']\n",
    "\n",
    "rents_aspects = ['LOCATION', 'PRICE', 'SERVICE#RENTING', 'SERVICE#STAFF', 'VEHICLE#QUALITY']\n",
    "\n",
    "tours_aspects = ['LOCATION',\n",
    " 'PRICE',\n",
    " 'SERVICE#STAFF',\n",
    " 'EXPERIENCE#ACTIVITY',\n",
    " 'ENVIRONMENT#SCENERY',\n",
    " 'ENVIRONMENT#ATMOSPHERE']\n",
    "\n",
    "campings_aspects = ['LOCATION#DISTANCE',\n",
    " 'LOCATION#ACCESSIBILITY',\n",
    " 'SERVICE#STAFF',\n",
    " 'ENVIRONMENT#SCENERY',\n",
    " 'ENVIRONMENT#WEATHER',\n",
    " 'ENVIRONMENT#ATMOSPHERE']\n",
    "\n",
    "sentiment_map = {\n",
    "    1: 'NEGATIVE',\n",
    "    2: 'NEUTRAL',\n",
    "    3: 'POSITIVE'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ed87764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "label_list = ['B-TOUR', 'B-RENT', 'B-RESTAURANT', 'I-RESTAURANT', 'B-ATTRACTION', 'I-TOUR', 'I-EATERY', 'O', 'B-HOTEL', 'I-HOTEL', 'I-ATTRACTION', 'I-CAMPING', 'B-EATERY', 'B-DRINKPLACE', 'I-RENT', 'B-CAMPING', 'I-DRINKPLACE']\n",
    "label_map = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for i, label in enumerate(label_list)}\n",
    "\n",
    "def merge_subwords(tokens, labels):\n",
    "    merged_tokens = []\n",
    "    merged_labels = []\n",
    "\n",
    "    current_token = \"\"\n",
    "    current_label = None\n",
    "\n",
    "    for token, label in zip(tokens, labels):\n",
    "        if token.endswith('@@'):  # Subword token\n",
    "            current_token += token[:-2]  # Bỏ @@ và nối vào\n",
    "            if current_label is None:\n",
    "                current_label = label  # Lấy label đầu tiên (B- hoặc I-)\n",
    "        else:\n",
    "            current_token += token  # Token đầy đủ, nối vào\n",
    "            if current_label is None:\n",
    "                current_label = label\n",
    "            merged_tokens.append(current_token)\n",
    "            merged_labels.append(current_label)\n",
    "            current_token = \"\"\n",
    "            current_label = None\n",
    "\n",
    "    # Nếu còn token cuối\n",
    "    if current_token:\n",
    "        merged_tokens.append(current_token)\n",
    "        merged_labels.append(current_label)\n",
    "\n",
    "    return merged_tokens, merged_labels\n",
    "\n",
    "\n",
    "def predict_ner(text, model, tokenizer, id2label):\n",
    "    # Tokenize\n",
    "    # text = text.lower()\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        is_split_into_words=False\n",
    "    )\n",
    "\n",
    "    # Predict\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    # Decode tokens và labels\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    labels = [id2label[label_id.item()] for label_id in predictions[0]]\n",
    "    # print(labels)\n",
    "    new_tokens, new_lables = merge_subwords(tokens[1:-1], labels[1:-1])\n",
    "    # Hiển thị sạch\n",
    "    # for token, label in zip(new_tokens, new_lables):\n",
    "    #     # token_clean = token.replace(\"▁\", \"\") if \"▁\" in token else token\n",
    "    #     # print(f\"{token_clean}\\t{label}\")\n",
    "    #     print(f\"{token}\\t{label}\")\n",
    "    return new_tokens, new_lables\n",
    "\n",
    "def extract_location_and_domain(tokens, labels):\n",
    "    target_labels = ['B-TOUR', 'B-RENT', 'B-RESTAURANT', \n",
    "                     'B-ATTRACTION', 'B-HOTEL', 'B-EATERY',\n",
    "                     'B-DRINKPLACE', 'B-CAMPING']\n",
    "    \n",
    "    extracted_tokens = []\n",
    "    domain = None\n",
    "    extracting = False\n",
    "\n",
    "    for token, label in zip(tokens, labels):\n",
    "        if not extracting:\n",
    "            if label in target_labels:\n",
    "                extracted_tokens.append(token)\n",
    "                domain = label.replace('B-', '')  # Lấy domain\n",
    "                extracting = True\n",
    "        else:\n",
    "            if label.startswith('I-'):\n",
    "                extracted_tokens.append(token)\n",
    "            else:\n",
    "                break  # Gặp O hoặc B- khác thì dừng\n",
    "\n",
    "    if extracted_tokens:\n",
    "        text = ' '.join(extracted_tokens)\n",
    "        return {\"text\": text, \"domain\": domain}\n",
    "    else:\n",
    "        return {\"text\": None, \"domain\": None}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93156959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_prediction(pred_array, aspects, sentiment_map):\n",
    "    pred_array = np.array(pred_array).flatten() \n",
    "\n",
    "    result = {}\n",
    "    for aspect, sentiment_id in zip(aspects, pred_array):\n",
    "        if sentiment_id != 0: \n",
    "            result[aspect] = sentiment_map[sentiment_id]\n",
    "\n",
    "    return result\n",
    "\n",
    "# decode_prediction(ypred_single, aspects, sentiment_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e7325b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(output_csv_path, text, place_extracted, domain_extracted, aspect_result):\n",
    "\n",
    "    record_result = {\n",
    "        'text': text,\n",
    "        'place_extracted': place_extracted,\n",
    "        'domain_extracted': domain_extracted,\n",
    "        'aspect_result': json.dumps(aspect_result, ensure_ascii=False)\n",
    "    }\n",
    "\n",
    "    df_new = pd.DataFrame([record_result])\n",
    "\n",
    "    if not os.path.exists(output_csv_path):\n",
    "        df_new.to_csv(output_csv_path, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(output_csv_path, mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ba0521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from kafka import KafkaConsumer, KafkaProducer\n",
    "# import json\n",
    "# import pickle\n",
    "\n",
    "# def ner_absa_consumer_processor(config, ner_model, ner_tokenizer,\n",
    "#                                 attractions_model, attractions_vectorizer,\n",
    "#                                 eateries_model, eateries_vectorizer,\n",
    "#                                 rents_model, rents_vectorizer,\n",
    "#                                 drinkplaces_model, drinkplaces_vectorizer,\n",
    "#                                 campings_model, campings_vectorizer, \n",
    "#                                 tours_model, tours_vectorizer,\n",
    "#                                 restaurants_model, restaurants_vectorizer,\n",
    "#                                 hotels_model, hotels_vectorizer):\n",
    "#     consumer = KafkaConsumer(\n",
    "#         config.KAFKA_TOPIC_COMMENTS,\n",
    "#         bootstrap_servers=config.KAFKA_SERVERS,\n",
    "#         value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
    "#         group_id='comments_viewer',\n",
    "#         auto_offset_reset='earliest',\n",
    "#     )\n",
    "\n",
    "#     # producer = KafkaProducer(\n",
    "#     #     bootstrap_servers=config.KAFKA_SERVERS,\n",
    "#     #     value_serializer=lambda v: json.dumps(v, ensure_ascii=False).encode('utf-8')\n",
    "#     # )\n",
    "\n",
    "#     for message in consumer:\n",
    "#         print(message.value)\n",
    "#         record = message.value\n",
    "#         text = record.get('text', '')\n",
    "\n",
    "#         predict_tokens, predict_labels = predict_ner(text, ner_model, ner_tokenizer, id2label)\n",
    "#         ner_result = extract_location_and_domain(predict_tokens, predict_labels)\n",
    "#         place_extracted = ner_result['text']\n",
    "#         domain_extracted = ner_result['domain']\n",
    "        \n",
    "#         if domain_extracted == 'TOUR':\n",
    "#             text_tfidf = tours_vectorizer.transform([text])  # dùng [text] để thành shape (1, n_features)\n",
    "#             ypred_single = tours_model.predict(text_tfidf)\n",
    "#             aspect_result = decode_prediction(ypred_single, tours_aspects, sentiment_map)\n",
    "#         elif domain_extracted == 'RENT':\n",
    "#             text_tfidf = rents_vectorizer.transform([text])  # dùng [text] để thành shape (1, n_features)\n",
    "#             ypred_single = rents_model.predict(text_tfidf)\n",
    "#             aspect_result = decode_prediction(ypred_single, rents_aspects, sentiment_map)\n",
    "#         elif domain_extracted == 'RESTAURANT':\n",
    "#             text_tfidf = restaurants_vectorizer.transform([text])  # dùng [text] để thành shape (1, n_features)\n",
    "#             ypred_single = restaurants_model.predict(text_tfidf)\n",
    "#             aspect_result = decode_prediction(ypred_single, restaurants_aspects, sentiment_map)\n",
    "#         elif domain_extracted == 'ATTRACTION':\n",
    "#             attractions_model, attractions_vectorizer\n",
    "#             text_tfidf = attractions_vectorizer.transform([text])  # dùng [text] để thành shape (1, n_features)\n",
    "#             ypred_single = attractions_model.predict(text_tfidf)\n",
    "#             aspect_result = decode_prediction(ypred_single, attractions_aspects, sentiment_map)\n",
    "#         elif domain_extracted == 'HOTEL':\n",
    "#             text_tfidf = hotels_vectorizer.transform([text])  # dùng [text] để thành shape (1, n_features)\n",
    "#             ypred_single = hotels_model.predict(text_tfidf)\n",
    "#             aspect_result = decode_prediction(ypred_single, hotels_aspects, sentiment_map)\n",
    "#         elif domain_extracted == 'EATERY':\n",
    "#             text_tfidf = eateries_vectorizer.transform([text])  # dùng [text] để thành shape (1, n_features)\n",
    "#             ypred_single = eateries_model.predict(text_tfidf)\n",
    "#             aspect_result = decode_prediction(ypred_single, eateries_aspects, sentiment_map)\n",
    "#         elif domain_extracted == 'DRINKPLACE':\n",
    "#             text_tfidf = drinkplaces_vectorizer.transform([text])  # dùng [text] để thành shape (1, n_features)\n",
    "#             ypred_single = drinkplaces_model.predict(text_tfidf)\n",
    "#             aspect_result = decode_prediction(ypred_single, drinkplaces_aspects, sentiment_map)\n",
    "#         elif domain_extracted == 'CAMPING':\n",
    "#             text_tfidf = campings_vectorizer.transform([text])  # dùng [text] để thành shape (1, n_features)\n",
    "#             ypred_single = campings_model.predict(text_tfidf)\n",
    "#             aspect_result = decode_prediction(ypred_single, campings_aspects, sentiment_map)\n",
    "            \n",
    "#         save_to_csv(r'F:\\Studies\\Third_year\\Big_data\\Final_Code\\Result\\result.csv', text, place_extracted, domain_extracted, aspect_result)\n",
    "            \n",
    "            \n",
    "#         # absa_result = absa_model.predict(text)\n",
    "\n",
    "\n",
    "#         # record['ner_result'] = ner_result\n",
    "#         # record['absa_result'] = absa_result\n",
    "\n",
    "#         # producer.send(config.KAFKA_TOPIC_PREDICTED, value=record)\n",
    "#         # producer.flush()\n",
    "\n",
    "#         # print(f\"[PROCESSOR] Processed and pushed record: {text[:30]}...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abc6738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = Config()\n",
    "# ner_model = AutoModelForTokenClassification.from_pretrained(config.MODEL_NER_PATH)\n",
    "# ner_tokenizer = AutoTokenizer.from_pretrained(config.MODEL_NER_PATH)\n",
    "\n",
    "# with open(config.ATTRACTIONS_ABSA_MODEL, 'rb') as f:\n",
    "#     attractions_package = pickle.load(f)\n",
    "# attractions_model = attractions_package['model']\n",
    "# attractions_vectorizer = attractions_package['vectorizer']\n",
    "\n",
    "# with open(config.HOTELS_ABSA_MODEL, 'rb') as f:\n",
    "#     hotels_package = pickle.load(f)\n",
    "# hotels_model = hotels_package['model']\n",
    "# hotels_vectorizer = hotels_package['vectorizer']\n",
    "\n",
    "# with open(config.RESTAURANTS_ABSA_MODEL, 'rb') as f:\n",
    "#     restaurants_package = pickle.load(f)\n",
    "# restaurants_model = restaurants_package['model']\n",
    "# restaurants_vectorizer = restaurants_package['vectorizer']\n",
    "\n",
    "# with open(config.DRINKPLACES_ABSA_MODEL, 'rb') as f:\n",
    "#     drinkplaces_package = pickle.load(f)\n",
    "# drinkplaces_model = drinkplaces_package['model']\n",
    "# drinkplaces_vectorizer = drinkplaces_package['vectorizer']\n",
    "\n",
    "# with open(config.EATERIES_ABSA_MODEL, 'rb') as f:\n",
    "#     eateries_package = pickle.load(f)\n",
    "# eateries_model = eateries_package['model']\n",
    "# eateries_vectorizer = eateries_package['vectorizer']\n",
    "\n",
    "# with open(config.RENTS_ABSA_MODEL, 'rb') as f:\n",
    "#     rents_package = pickle.load(f)\n",
    "# rents_model = rents_package['model']\n",
    "# rents_vectorizer = rents_package['vectorizer']\n",
    "\n",
    "# with open(config.TOURS_ABSA_MODEL, 'rb') as f:\n",
    "#     tours_package = pickle.load(f)\n",
    "# tours_model = tours_package['model']\n",
    "# tours_vectorizer = tours_package['vectorizer']\n",
    "\n",
    "# with open(config.CAMPINGS_ABSA_MODEL, 'rb') as f:\n",
    "#     campings_package = pickle.load(f)\n",
    "# campings_model = campings_package['model']\n",
    "# campings_vectorizer = campings_package['vectorizer']\n",
    "\n",
    "\n",
    "# ner_absa_consumer_processor(config, ner_model, ner_tokenizer,\n",
    "#                                 attractions_model, attractions_vectorizer,\n",
    "#                                 eateries_model, eateries_vectorizer,\n",
    "#                                 rents_model, rents_vectorizer,\n",
    "#                                 drinkplaces_model, drinkplaces_vectorizer,\n",
    "#                                 campings_model, campings_vectorizer, \n",
    "#                                 tours_model, tours_vectorizer,\n",
    "#                                 restaurants_model, restaurants_vectorizer,\n",
    "#                                 hotels_model, hotels_vectorizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c508a3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "ner_model = AutoModelForTokenClassification.from_pretrained(config.MODEL_NER_PATH)\n",
    "ner_tokenizer = AutoTokenizer.from_pretrained(config.MODEL_NER_PATH)\n",
    "\n",
    "with open(config.ATTRACTIONS_ABSA_MODEL, 'rb') as f:\n",
    "    attractions_package = pickle.load(f)\n",
    "attractions_model = attractions_package['model']\n",
    "attractions_vectorizer = attractions_package['vectorizer']\n",
    "\n",
    "with open(config.HOTELS_ABSA_MODEL, 'rb') as f:\n",
    "    hotels_package = pickle.load(f)\n",
    "hotels_model = hotels_package['model']\n",
    "hotels_vectorizer = hotels_package['vectorizer']\n",
    "\n",
    "with open(config.RESTAURANTS_ABSA_MODEL, 'rb') as f:\n",
    "    restaurants_package = pickle.load(f)\n",
    "restaurants_model = restaurants_package['model']\n",
    "restaurants_vectorizer = restaurants_package['vectorizer']\n",
    "\n",
    "with open(config.DRINKPLACES_ABSA_MODEL, 'rb') as f:\n",
    "    drinkplaces_package = pickle.load(f)\n",
    "drinkplaces_model = drinkplaces_package['model']\n",
    "drinkplaces_vectorizer = drinkplaces_package['vectorizer']\n",
    "\n",
    "with open(config.EATERIES_ABSA_MODEL, 'rb') as f:\n",
    "    eateries_package = pickle.load(f)\n",
    "eateries_model = eateries_package['model']\n",
    "eateries_vectorizer = eateries_package['vectorizer']\n",
    "\n",
    "with open(config.RENTS_ABSA_MODEL, 'rb') as f:\n",
    "    rents_package = pickle.load(f)\n",
    "rents_model = rents_package['model']\n",
    "rents_vectorizer = rents_package['vectorizer']\n",
    "\n",
    "with open(config.TOURS_ABSA_MODEL, 'rb') as f:\n",
    "    tours_package = pickle.load(f)\n",
    "tours_model = tours_package['model']\n",
    "tours_vectorizer = tours_package['vectorizer']\n",
    "\n",
    "with open(config.CAMPINGS_ABSA_MODEL, 'rb') as f:\n",
    "    campings_package = pickle.load(f)\n",
    "campings_model = campings_package['model']\n",
    "campings_vectorizer = campings_package['vectorizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "369b3125",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "import json\n",
    "\n",
    "# Hàm UDF cho xử lý NER và ABSA\n",
    "def ner_absa_udf(text):\n",
    "    predict_tokens, predict_labels = predict_ner(text, ner_model, ner_tokenizer, id2label)\n",
    "    ner_result = extract_location_and_domain(predict_tokens, predict_labels)\n",
    "    place_extracted = ner_result['text']\n",
    "    domain_extracted = ner_result['domain']\n",
    "\n",
    "    if domain_extracted == 'TOUR':\n",
    "        vector = tours_vectorizer.transform([text])\n",
    "        prediction = tours_model.predict(vector)\n",
    "        aspect_result = decode_prediction(prediction, tours_aspects, sentiment_map)\n",
    "    elif domain_extracted == 'RENT':\n",
    "        vector = rents_vectorizer.transform([text])\n",
    "        prediction = rents_model.predict(vector)\n",
    "        aspect_result = decode_prediction(prediction, rents_aspects, sentiment_map)\n",
    "    elif domain_extracted == 'RESTAURANT':\n",
    "        vector = restaurants_vectorizer.transform([text])\n",
    "        prediction = restaurants_model.predict(vector)\n",
    "        aspect_result = decode_prediction(prediction, restaurants_aspects, sentiment_map)\n",
    "    elif domain_extracted == 'ATTRACTION':\n",
    "        vector = attractions_vectorizer.transform([text])\n",
    "        prediction = attractions_model.predict(vector)\n",
    "        aspect_result = decode_prediction(prediction, attractions_aspects, sentiment_map)\n",
    "    elif domain_extracted == 'HOTEL':\n",
    "        vector = hotels_vectorizer.transform([text])\n",
    "        prediction = hotels_model.predict(vector)\n",
    "        aspect_result = decode_prediction(prediction, hotels_aspects, sentiment_map)\n",
    "    elif domain_extracted == 'EATERY':\n",
    "        vector = eateries_vectorizer.transform([text])\n",
    "        prediction = eateries_model.predict(vector)\n",
    "        aspect_result = decode_prediction(prediction, eateries_aspects, sentiment_map)\n",
    "    elif domain_extracted == 'DRINKPLACE':\n",
    "        vector = drinkplaces_vectorizer.transform([text])\n",
    "        prediction = drinkplaces_model.predict(vector)\n",
    "        aspect_result = decode_prediction(prediction, drinkplaces_aspects, sentiment_map)\n",
    "    elif domain_extracted == 'CAMPING':\n",
    "        vector = campings_vectorizer.transform([text])\n",
    "        prediction = campings_model.predict(vector)\n",
    "        aspect_result = decode_prediction(prediction, campings_aspects, sentiment_map)\n",
    "    else:\n",
    "        place_extracted = ''\n",
    "        domain_extracted = ''\n",
    "        aspect_result = {}\n",
    "\n",
    "    # Trả kết quả dạng chuỗi JSON để lưu thẳng CSV\n",
    "    return json.dumps({\n",
    "        \"place\": place_extracted,\n",
    "        \"domain\": domain_extracted,\n",
    "        \"aspect_result\": aspect_result\n",
    "    }, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2b16538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dfa9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall pyspark\n",
    "!pip uninstall findspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b843bc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark findspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6148a45",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'JavaPackage' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Tạo SparkSession\u001b[39;00m\n\u001b[32m      2\u001b[39m spark = \u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mNER_ABSA_Streaming\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Đăng ký UDF\u001b[39;00m\n\u001b[32m      7\u001b[39m ner_absa_predict_udf = udf(ner_absa_udf, StringType())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADMIN\\.conda\\envs\\prj__env\\Lib\\site-packages\\pyspark\\sql\\session.py:559\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    556\u001b[39m     sc = SparkContext.getOrCreate(sparkConf)\n\u001b[32m    557\u001b[39m     \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    558\u001b[39m     \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m559\u001b[39m     session = \u001b[43mSparkSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    561\u001b[39m     module = SparkSession._get_j_spark_session_module(session._jvm)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADMIN\\.conda\\envs\\prj__env\\Lib\\site-packages\\pyspark\\sql\\session.py:635\u001b[39m, in \u001b[36mSparkSession.__init__\u001b[39m\u001b[34m(self, sparkContext, jsparkSession, options)\u001b[39m\n\u001b[32m    631\u001b[39m jSparkSessionModule = SparkSession._get_j_spark_session_module(\u001b[38;5;28mself\u001b[39m._jvm)\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m jsparkSession \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    634\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m635\u001b[39m         \u001b[43mjSparkSessionClass\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetDefaultSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.isDefined()\n\u001b[32m    636\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m jSparkSessionClass.getDefaultSession().get().sparkContext().isStopped()\n\u001b[32m    637\u001b[39m     ):\n\u001b[32m    638\u001b[39m         jsparkSession = jSparkSessionClass.getDefaultSession().get()\n\u001b[32m    639\u001b[39m         jSparkSessionModule.applyModifiableSettings(jsparkSession, options)\n",
      "\u001b[31mTypeError\u001b[39m: 'JavaPackage' object is not callable"
     ]
    }
   ],
   "source": [
    "# Tạo SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NER_ABSA_Streaming\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Đăng ký UDF\n",
    "ner_absa_predict_udf = udf(ner_absa_udf, StringType())\n",
    "\n",
    "# Đọc dữ liệu từ Kafka\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", config.KAFKA_SERVERS) \\\n",
    "    .option(\"subscribe\", config.KAFKA_TOPIC_COMMENTS) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Giải mã và chọn trường text\n",
    "df_text = df.selectExpr(\"CAST(value AS STRING) as json_str\")\n",
    "df_parsed = df_text.selectExpr(\"from_json(json_str, 'text STRING') as data\").select(\"data.text\")\n",
    "\n",
    "# Dự đoán hàng loạt (batch)\n",
    "df_with_prediction = df_parsed.withColumn(\"ner_absa_result\", ner_absa_predict_udf(col(\"text\")))\n",
    "\n",
    "# Ghi kết quả ra CSV\n",
    "query = df_with_prediction.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"path\", r\"F:\\Studies\\Third_year\\Big_data\\Final_Code\\Result\") \\\n",
    "    .option(\"checkpointLocation\", r\"F:\\Studies\\Third_year\\Big_data\\Final_Code\\Checkpoint\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3779857",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "java version \"21.0.6\" 2025-01-21 LTS\n",
      "Java(TM) SE Runtime Environment (build 21.0.6+8-LTS-188)\n",
      "Java HotSpot(TM) 64-Bit Server VM (build 21.0.6+8-LTS-188, mixed mode, sharing)\n"
     ]
    }
   ],
   "source": [
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3bc268",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prj__env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
