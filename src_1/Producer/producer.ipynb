{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a31fe912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apify_client import ApifyClient\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d420be15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    API_TOKEN            = \"apify_api_UzQNmO6WMVXVZbIDce889C6er7Rxv92zpmuE\"\n",
    "    URL                  = \"https://www.facebook.com/asiantrip.dulich\"\n",
    "    RESULTS_LIMIT        = 3\n",
    "    KAFKA_SERVERS        = ['localhost:9092']\n",
    "    KAFKA_TOPIC_FB_POSTS = 'fb_posts'\n",
    "    KAFKA_TOPIC_COMMENTS = 'comments'\n",
    "    GOOGLE_API_KEY       = \"AIzaSyCouQqHptWcbbnbzbVQjICFgwMEoYhDv5o\"\n",
    "    LLM_NAME             = \"gemini-1.5-flash-latest\"\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fd283fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_apify_facebook_crawl_and_save_json(config):\n",
    "#     \"\"\"\n",
    "#     Args:\n",
    "#         api_token (str): Apify API token.\n",
    "#         url (str): URL Facebook page cần crawl.\n",
    "#         results_limit (int): Giới hạn số bài post cần crawl.\n",
    "#         output_file (str): Tên file JSON để lưu kết quả.\n",
    "#     \"\"\"\n",
    "#     # Initialize the ApifyClient with your API token\n",
    "#     client = ApifyClient(config.API_TOKEN)\n",
    "\n",
    "#     # Prepare the Actor input\n",
    "#     run_input = {\n",
    "#         \"startUrls\": [{ \"url\": config.URL }],\n",
    "#         \"resultsLimit\": config.RESULTS_LIMIT,\n",
    "#         \"captionText\": False,\n",
    "#     }\n",
    "\n",
    "#     # Run the Actor and wait for it to finish\n",
    "#     run = client.actor(\"KoJrdxJCTtpon81KY\").call(run_input=run_input)\n",
    "\n",
    "#     # Fetch results and save to JSON file\n",
    "#     results = []\n",
    "#     for item in client.dataset(run[\"defaultDatasetId\"]).iterate_items():\n",
    "#         results.append(item)\n",
    "    \n",
    "#     save_file = os.path.join(config.OUTPUT_FILE, 'run_1.json')\n",
    "#     with open(save_file, \"w\", encoding=\"utf-8\") as f:\n",
    "#         json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "#     print(f\"✅ Đã lưu {len(results)} kết quả vào {save_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5193b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_apify_facebook_crawl_push_kafka_topic(config):\n",
    "#     \"\"\"\n",
    "#     Crawl Facebook posts using Apify and push results to Kafka topic 'fb_posts'.\n",
    "\n",
    "#     Args:\n",
    "#         config: An object with attributes:\n",
    "#             API_TOKEN (str): Apify API token.\n",
    "#             URL (str): URL Facebook page cần crawl.\n",
    "#             RESULTS_LIMIT (int): Giới hạn số bài post cần crawl.\n",
    "#             KAFKA_SERVERS (List[str]): List Kafka bootstrap servers, e.g., ['localhost:9092'].\n",
    "#             KAFKA_TOPIC (str): Kafka topic name to push, e.g., 'fb_posts'.\n",
    "#     \"\"\"\n",
    "#     # Initialize the ApifyClient with your API token\n",
    "#     client = ApifyClient(config.API_TOKEN)\n",
    "\n",
    "#     # Initialize Kafka Producer\n",
    "#     producer = KafkaProducer(\n",
    "#         bootstrap_servers=config.KAFKA_SERVERS,\n",
    "#         value_serializer=lambda v: json.dumps(v, ensure_ascii=False).encode('utf-8')\n",
    "#     )\n",
    "\n",
    "#     try:\n",
    "#         while True:\n",
    "#             if os.path.exists(\"stop_flag\"):\n",
    "#                 print(\"[SYSTEM] Detected 'stop_flag'. Stopping gracefully.\")\n",
    "#                 break\n",
    "\n",
    "#             print(\"[SYSTEM] Starting a new crawl cycle.\")\n",
    "\n",
    "#             run_input = {\n",
    "#                 \"startUrls\": [{\"url\": config.URL}],\n",
    "#                 \"resultsLimit\": config.RESULTS_LIMIT,\n",
    "#                 \"captionText\": False,\n",
    "#             }\n",
    "\n",
    "#             run = client.actor(\"KoJrdxJCTtpon81KY\").call(run_input=run_input)\n",
    "\n",
    "#             for item in client.dataset(run[\"defaultDatasetId\"]).iterate_items():\n",
    "#                 fb_post = {\n",
    "#                     \"text\": item.get(\"text\"),\n",
    "#                     \"url\": item.get(\"url\")\n",
    "#                 }\n",
    "#                 producer.send(config.KAFKA_TOPIC, value=fb_post)\n",
    "#                 print(f\"✅ Pushed post: {fb_post['text'][:30]}... to Kafka topic {config.KAFKA_TOPIC}\")\n",
    "\n",
    "#             producer.flush()\n",
    "#             print(\"[SYSTEM] Completed pushing all crawled posts to Kafka.\")\n",
    "\n",
    "#             print(\"[SYSTEM] Sleeping for 30 minutes before next crawl...\")\n",
    "#             for _ in range(30 * 60):\n",
    "#                 if os.path.exists(\"stop_flag\"):\n",
    "#                     print(\"[SYSTEM] Detected 'stop_flag' during sleep. Stopping gracefully.\")\n",
    "#                     raise KeyboardInterrupt\n",
    "#                 time.sleep(1)\n",
    "\n",
    "#     except KeyboardInterrupt:\n",
    "#         print(\"[SYSTEM] Stopping gracefully.\")\n",
    "#     finally:\n",
    "#         producer.flush()\n",
    "#         producer.close()\n",
    "#         print(\"[SYSTEM] Producer closed cleanly.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "585fbd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_apify_crawl_push_batch(config):\n",
    "    \"\"\"\n",
    "    Apify crawl Facebook posts, push 2 records every 30 minutes to server.\n",
    "    \"\"\"\n",
    "\n",
    "    client = ApifyClient(config.API_TOKEN)\n",
    "\n",
    "    # Crawl 1 lần duy nhất\n",
    "    run_input = {\n",
    "        \"startUrls\": [{\"url\": config.URL}],\n",
    "        \"resultsLimit\": config.RESULTS_LIMIT,\n",
    "        \"captionText\": False,\n",
    "    }\n",
    "    print(\"[SYSTEM] Running Apify crawl...\")\n",
    "    run = client.actor(\"KoJrdxJCTtpon81KY\").call(run_input=run_input)\n",
    "    data_list = []\n",
    "\n",
    "    for item in client.dataset(run[\"defaultDatasetId\"]).iterate_items():\n",
    "        fb_post = {\n",
    "            \"text\": item.get(\"text\"),\n",
    "            \"url\": item.get(\"url\")\n",
    "        }\n",
    "        data_list.append(fb_post)\n",
    "\n",
    "    print(f\"[SYSTEM] Crawl completed, {len(data_list)} records fetched.\")\n",
    "\n",
    "    producer = KafkaProducer(\n",
    "        bootstrap_servers=config.KAFKA_SERVERS,\n",
    "        value_serializer=lambda v: json.dumps(v, ensure_ascii=False).encode('utf-8')\n",
    "    )\n",
    "\n",
    "    index = 0\n",
    "    try:\n",
    "        while index < len(data_list):\n",
    "            batch = data_list[index:index+3]\n",
    "\n",
    "            for record in batch:\n",
    "                producer.send(config.KAFKA_TOPIC_FB_POSTS, value=record)\n",
    "                # print(f\"✅ Pushed: {record['text'][:30]}... to {config.KAFKA_TOPIC_FB_POSTS}\")\n",
    "\n",
    "            producer.flush()\n",
    "            index += 3\n",
    "\n",
    "            if index >= len(data_list):\n",
    "                print(\"[SYSTEM] All records have been pushed.\")\n",
    "                break\n",
    "\n",
    "            print(\"[SYSTEM] Sleeping 30 minutes before pushing next 3 records...\")\n",
    "            time.sleep(30 * 60)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"[SYSTEM] Stopped by user.\")\n",
    "    finally:\n",
    "        producer.flush()\n",
    "        producer.close()\n",
    "        print(\"[SYSTEM] Producer closed cleanly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fac99834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SYSTEM] Running Apify crawl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.facebook-posts-scraper runId:AmDrzhRxSA9lhBjhl]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.facebook-posts-scraper runId:AmDrzhRxSA9lhBjhl]\u001b[0m -> 2025-07-01T16:11:50.343Z ACTOR: Pulling Docker image of build gqbIH3NP8pt1y5Y8w from registry.\n",
      "\u001b[36m[apify.facebook-posts-scraper runId:AmDrzhRxSA9lhBjhl]\u001b[0m -> 2025-07-01T16:11:50.346Z ACTOR: Creating Docker container.\n",
      "\u001b[36m[apify.facebook-posts-scraper runId:AmDrzhRxSA9lhBjhl]\u001b[0m -> 2025-07-01T16:11:50.515Z ACTOR: Starting Docker container.\n",
      "\u001b[36m[apify.facebook-posts-scraper runId:AmDrzhRxSA9lhBjhl]\u001b[0m -> 2025-07-01T16:11:52.140Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.2\",\"apifyClientVersion\":\"2.12.5\",\"crawleeVersion\":\"3.13.7\",\"osType\":\"Linux\",\"nodeVersion\":\"v20.19.2\"}\u001b[39m\n",
      "\u001b[36m[apify.facebook-posts-scraper runId:AmDrzhRxSA9lhBjhl]\u001b[0m -> 2025-07-01T16:11:52.270Z \u001b[32mINFO\u001b[39m  Results Limit 3\n",
      "\u001b[36m[apify.facebook-posts-scraper runId:AmDrzhRxSA9lhBjhl]\u001b[0m -> 2025-07-01T16:11:52.593Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Using the old RequestQueue implementation without request locking.\n",
      "\u001b[36m[apify.facebook-posts-scraper runId:AmDrzhRxSA9lhBjhl]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.facebook-posts-scraper runId:AmDrzhRxSA9lhBjhl]\u001b[0m -> 2025-07-01T16:11:52.848Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.facebook-posts-scraper runId:AmDrzhRxSA9lhBjhl]\u001b[0m -> Status: RUNNING, Message: Crawled 1/2 pages, 0 failed requests, desired concurrency 10.\n",
      "\u001b[36m[apify.facebook-posts-scraper runId:AmDrzhRxSA9lhBjhl]\u001b[0m -> 2025-07-01T16:12:01.633Z \u001b[32mINFO\u001b[39m  https://www.facebook.com/asiantrip.dulich resolved with Facebook ID 100082913303638\n",
      "\u001b[36m[apify.facebook-posts-scraper runId:AmDrzhRxSA9lhBjhl]\u001b[0m -> 2025-07-01T16:12:04.715Z \u001b[32mINFO\u001b[39m  Stopping extraction for page posts because max posts were reached https://www.facebook.com/asiantrip.dulich\n",
      "\u001b[36m[apify.facebook-posts-scraper runId:AmDrzhRxSA9lhBjhl]\u001b[0m -> 2025-07-01T16:12:04.717Z \u001b[32mINFO\u001b[39m  [DONE]: end of results for 100082913303638 at position 3\n",
      "\u001b[36m[apify.facebook-posts-scraper runId:AmDrzhRxSA9lhBjhl]\u001b[0m -> 2025-07-01T16:12:04.816Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.facebook-posts-scraper runId:AmDrzhRxSA9lhBjhl]\u001b[0m -> 2025-07-01T16:12:04.930Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":2,\"requestsFailed\":0,\"retryHistogram\":[2],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":5875,\"requestsFinishedPerMinute\":10,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":11749,\"requestsTotal\":2,\"crawlerRuntimeMillis\":12337}\u001b[39m\n",
      "\u001b[36m[apify.facebook-posts-scraper runId:AmDrzhRxSA9lhBjhl]\u001b[0m -> 2025-07-01T16:12:04.932Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Finished! Total 2 requests: 2 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.facebook-posts-scraper runId:AmDrzhRxSA9lhBjhl]\u001b[0m -> 2025-07-01T16:12:05.106Z \u001b[32mINFO\u001b[39m  *** DONE ***\n",
      "\u001b[36m[apify.facebook-posts-scraper runId:AmDrzhRxSA9lhBjhl]\u001b[0m -> Status: SUCCEEDED, Message: Finished! Total 2 requests: 2 succeeded, 0 failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SYSTEM] Crawl completed, 3 records fetched.\n",
      "[SYSTEM] All records have been pushed.\n",
      "[SYSTEM] Producer closed cleanly.\n"
     ]
    }
   ],
   "source": [
    "run_apify_crawl_push_batch(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "438e2db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['localhost:9092']\n"
     ]
    }
   ],
   "source": [
    "print(config.KAFKA_SERVERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27d6656a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4b179dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_name = 'items'\n",
    "kafka_server = 'localhost:9092'\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers=kafka_server)\n",
    "\n",
    "producer.send(topic_name, b'Hello Trung')\n",
    "producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2c43f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
