{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DMNLuRRfsEc2"
      },
      "outputs": [],
      "source": [
        "!pip install -qq datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73wOnPUBsMPT",
        "outputId": "29a7c8a3-ec75-4eab-dfe8-4c3132cd0b5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique labels found: {'I-RENTALSERVICES', 'B-RESTAURANTS', 'I-HOTELS', 'O', 'B-HOTELS', 'B-DRINKPLACES', 'I-DRINKPLACES', 'B-RENTALSERVICES', 'B-STREETFOODRESTAURANT', 'I-RESTAURANTS', 'I-ATTRACTIONS', 'I-STREETFOODRESTAURANT', 'B-ATTRACTIONS', 'I-TOUR', 'B-TOUR'}\n"
          ]
        }
      ],
      "source": [
        "def read_conll(file_path):\n",
        "    sentences = []\n",
        "    sentence_labels = []\n",
        "    unique_labels = set()  # To collect unique labels\n",
        "\n",
        "    with open(file_path, 'r') as file:\n",
        "        current_sentence_tokens = []\n",
        "        current_sentence_labels = []\n",
        "\n",
        "        for line in file:\n",
        "            line = line.strip()  # Remove leading/trailing whitespace, including '\\n'\n",
        "\n",
        "            # If it's an empty line, sentence boundary detected\n",
        "            if not line:\n",
        "                if current_sentence_tokens:  # Check if there's a sentence to append\n",
        "                    sentences.append(' '.join(current_sentence_tokens))\n",
        "                    sentence_labels.append(' '.join(current_sentence_labels))\n",
        "                current_sentence_tokens = []  # Reset for the next sentence\n",
        "                current_sentence_labels = []  # Reset for the next sentence\n",
        "            else:\n",
        "                line_parts = line.split()  # Split line into token and label\n",
        "                current_sentence_tokens.append(line_parts[0])\n",
        "\n",
        "                if len(line_parts) >= 2:\n",
        "                    current_sentence_labels.append(line_parts[1])\n",
        "                    unique_labels.add(line_parts[1])  # Add label to the set of unique labels\n",
        "                else:\n",
        "                    current_sentence_labels.append('O')  # Default to 'O' if no label provided\n",
        "\n",
        "    # Append the last sentence if the file doesn't end with an empty line\n",
        "    if current_sentence_tokens:\n",
        "        sentences.append(' '.join(current_sentence_tokens))\n",
        "        sentence_labels.append(' '.join(current_sentence_labels))\n",
        "\n",
        "    print(f\"Unique labels found: {unique_labels}\")\n",
        "    return sentences, sentence_labels\n",
        "\n",
        "# Load the datasets\n",
        "# test_sentences, test_labels = read_conll('./test_word.conll')\n",
        "# dev_sentences, dev_labels = read_conll('./dev_word.conll')\n",
        "train_sentences, train_labels = read_conll('/content/da_lat_ner_1000_records_precise.conll')\n",
        "\n",
        "# Now, test_sentences, test_labels, dev_sentences, dev_labels, train_sentences, and train_labels are arrays of strings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qOcbTgpBsOOt",
        "outputId": "da46a4b7-5416-4d13-bf01-b2bbb21bac9c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Hôm nay đi Tiệm Trà Cúc , cảnh rất đẹp và yên bình .'"
            ]
          },
          "execution_count": 124,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_sentences[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WaW0CwsrsQNt",
        "outputId": "b073f0a8-99ed-446f-da06-172c76450cda"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'O O O B-DRINKPLACES I-DRINKPLACES I-DRINKPLACES O O O O O O O O'"
            ]
          },
          "execution_count": 125,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_labels[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wV1jj1MTsvgk",
        "outputId": "91967f09-f6c0-472c-b0cb-edb01dc722da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset size: 1000\n",
            "Train dataset sample: {'tokens': ['Checkin', 'ở', 'Nhà', 'hàng', 'Song', 'May', ',', 'sống', 'ảo', 'cả', 'buổi', 'không', 'chán', '.'], 'labels': ['O', 'O', 'B-RESTAURANTS', 'I-RESTAURANTS', 'I-RESTAURANTS', 'I-RESTAURANTS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}\n"
          ]
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# Step 1: Prepare the datasets from sentences and labels\n",
        "def prepare_dataset(sentences, labels):\n",
        "    return {'tokens': sentences, 'labels': labels}\n",
        "\n",
        "train_dataset = prepare_dataset(train_sentences, train_labels)\n",
        "# dev_dataset = prepare_dataset(dev_sentences, dev_labels)\n",
        "# test_dataset = prepare_dataset(test_sentences, test_labels)\n",
        "\n",
        "# Step 2: Convert strings of tokens and labels into arrays\n",
        "def process_string_to_array(dataset):\n",
        "    return {\n",
        "        'tokens': [sentence.split() for sentence in dataset['tokens']],\n",
        "        'labels': [label_seq.split() for label_seq in dataset['labels']]\n",
        "    }\n",
        "\n",
        "# Step 3: Process the dataset for token and label lists\n",
        "train_dataset = process_string_to_array(train_dataset)\n",
        "# dev_dataset = process_string_to_array(dev_dataset)\n",
        "# test_dataset = process_string_to_array(test_dataset)\n",
        "\n",
        "# Step 4: Convert processed datasets into Hugging Face Dataset objects\n",
        "train_dataset = Dataset.from_dict(train_dataset)\n",
        "# dev_dataset = Dataset.from_dict(dev_dataset)\n",
        "# test_dataset = Dataset.from_dict(test_dataset)\n",
        "\n",
        "# Print the size of each dataset and a sample for verification\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "# print(f\"Dev dataset size: {len(dev_dataset)}\")\n",
        "# print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "print(\"Train dataset sample:\", train_dataset[0])\n",
        "# print(\"Dev dataset sample:\", dev_dataset[0])\n",
        "# print(\"Test dataset sample:\", test_dataset[0])\n",
        "\n",
        "# Step 5: Define an Example class\n",
        "class Example:\n",
        "    def __init__(self, words, slot_labels, guid=None):\n",
        "        self.words = words\n",
        "        self.slot_labels = slot_labels\n",
        "        self.guid = guid\n",
        "\n",
        "# Step 6: Convert the dataset to Example objects\n",
        "def convert_to_examples(dataset):\n",
        "    return [\n",
        "        Example(words=tokens, slot_labels=labels, guid=i)\n",
        "        for i, (tokens, labels) in enumerate(zip(dataset['tokens'], dataset['labels']))\n",
        "    ]\n",
        "\n",
        "# Convert datasets into Example objects\n",
        "train_examples = convert_to_examples(train_dataset)\n",
        "# dev_examples = convert_to_examples(dev_dataset)\n",
        "# test_examples = convert_to_examples(test_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "-LwtAryKsxaH"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "import copy\n",
        "import json\n",
        "import logging\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "SkeE1Mj-vRs0"
      },
      "outputs": [],
      "source": [
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, attention_mask, token_type_ids, slot_labels_ids):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.token_type_ids = token_type_ids\n",
        "        self.slot_labels_ids = slot_labels_ids\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.to_json_string())\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "        output = copy.deepcopy(self.__dict__)\n",
        "        return output\n",
        "\n",
        "    def to_json_string(self):\n",
        "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "sO0Jdpd2szLF"
      },
      "outputs": [],
      "source": [
        "def convert_examples_to_features(\n",
        "    examples,\n",
        "    max_seq_len,\n",
        "    tokenizer,\n",
        "    pad_label_id=-100,\n",
        "    cls_token_segment_id=0,\n",
        "    pad_token_segment_id=0,\n",
        "    sequence_segment_id=0,\n",
        "    mask_padding_with_zero=True,\n",
        "):\n",
        "    # Get special tokens from the tokenizer\n",
        "    cls_token = tokenizer.cls_token\n",
        "    sep_token = tokenizer.sep_token\n",
        "    unk_token = tokenizer.unk_token\n",
        "    pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    # List to hold the converted features\n",
        "    features = []\n",
        "\n",
        "    for example_index, example in enumerate(examples):\n",
        "        # Log progress every 5000 examples\n",
        "        if example_index % 400 == 0:\n",
        "            logger.info(f\"Processing example {example_index} of {len(examples)}\")\n",
        "\n",
        "        # Tokenize each word and align its corresponding label\n",
        "        tokens = []\n",
        "        label_ids = []\n",
        "\n",
        "        for word, label in zip(example.words, example.slot_labels):\n",
        "            word_tokens = tokenizer.tokenize(word)\n",
        "\n",
        "            # If the word cannot be tokenized, use [UNK] token\n",
        "            if not word_tokens:\n",
        "                word_tokens = [unk_token]\n",
        "\n",
        "            tokens.extend(word_tokens)\n",
        "\n",
        "            # Map string label to integer ID, apply pad_label_id for subword tokens\n",
        "            label_id = label_map[label]\n",
        "            label_ids.extend([label_id] + [pad_label_id] * (len(word_tokens) - 1))\n",
        "\n",
        "        # Handle sequence truncation for [CLS] and [SEP] tokens\n",
        "        special_tokens_count = 2\n",
        "        if len(tokens) > max_seq_len - special_tokens_count:\n",
        "            tokens = tokens[:max_seq_len - special_tokens_count]\n",
        "            label_ids = label_ids[:max_seq_len - special_tokens_count]\n",
        "\n",
        "        # Add [SEP] token at the end of the sentence\n",
        "        tokens.append(sep_token)\n",
        "        label_ids.append(pad_label_id)\n",
        "        token_type_ids = [sequence_segment_id] * len(tokens)\n",
        "\n",
        "        # Add [CLS] token at the start of the sentence\n",
        "        tokens = [cls_token] + tokens\n",
        "        label_ids = [pad_label_id] + label_ids\n",
        "        token_type_ids = [cls_token_segment_id] + token_type_ids\n",
        "\n",
        "        # Convert tokens to input IDs\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # Create attention masks (1 for real tokens, 0 for padding tokens)\n",
        "        attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
        "\n",
        "\n",
        "        # Pad sequences to the maximum sequence length\n",
        "        padding_length = max_seq_len - len(input_ids)\n",
        "        input_ids += [pad_token_id] * padding_length\n",
        "        attention_mask += [0 if mask_padding_with_zero else 1] * padding_length\n",
        "        token_type_ids += [pad_token_segment_id] * padding_length\n",
        "        label_ids += [pad_label_id] * padding_length\n",
        "\n",
        "        # Create InputFeatures object and append it to the list of features\n",
        "        features.append(\n",
        "            InputFeatures(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                token_type_ids=token_type_ids,\n",
        "                slot_labels_ids=label_ids,\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "qsfSPrIEs1Ti"
      },
      "outputs": [],
      "source": [
        "# Define the label list (ensure that it includes all labels from your dataset)\n",
        "label_list = ['I-RENTALSERVICES', 'I-STREETFOODRESTAURANT', 'I-TOUR', 'B-HOTELS', 'I-RESTAURANTS', 'B-STREETFOODRESTAURANT', 'I-DRINKPLACES', 'I-HOTELS', 'B-RESTAURANTS', 'B-DRINKPLACES', 'I-ATTRACTIONS', 'B-ATTRACTIONS', 'O', 'B-RENTALSERVICES', 'B-TOUR']\n",
        "\n",
        "# Create a mapping from label strings to integers\n",
        "label_map = {label: i for i, label in enumerate(label_list)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "2Fm4e0PxtKow"
      },
      "outputs": [],
      "source": [
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "6YNArKpHtMLm"
      },
      "outputs": [],
      "source": [
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, attention_mask, token_type_ids, slot_labels_ids):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.token_type_ids = token_type_ids\n",
        "        self.slot_labels_ids = slot_labels_ids\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.to_json_string())\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "        output = copy.deepcopy(self.__dict__)\n",
        "        return output\n",
        "\n",
        "    def to_json_string(self):\n",
        "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "bawmhT1OtRSx"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Initialize PhoBERT tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"quocanh944/phoBERT-ner\")\n",
        "\n",
        "# Set the maximum sequence length\n",
        "max_seq_len = 128  # You can adjust this based on your model/input\n",
        "\n",
        "# Convert examples to features\n",
        "train_features = convert_examples_to_features(train_examples, max_seq_len, tokenizer)\n",
        "# dev_features = convert_examples_to_features(dev_examples, max_seq_len, tokenizer)\n",
        "# test_features = convert_examples_to_features(test_examples, max_seq_len, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTPel_FFvZM9",
        "outputId": "7f373a62-5c9c-4ef3-afee-f706d4983cb5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('<s>', '</s>', '<unk>', 1)"
            ]
          },
          "execution_count": 134,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.cls_token, tokenizer.sep_token, tokenizer.unk_token, tokenizer.pad_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "N2q1WIvotUqz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# Define a Dataset class to wrap the tokenized features for training\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, features):\n",
        "        self.features = features\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        feature = self.features[idx]\n",
        "        return {\n",
        "            'input_ids': torch.tensor(feature.input_ids, dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(feature.attention_mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(feature.token_type_ids, dtype=torch.long),\n",
        "            'labels': torch.tensor(feature.slot_labels_ids, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "# Convert tokenized features into PyTorch datasets\n",
        "train_dataset = NERDataset(train_features)\n",
        "# dev_dataset = NERDataset(dev_features)\n",
        "# test_dataset = NERDataset(test_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJm5d5w5tXHc",
        "outputId": "65b43bae-1a1e-4a16-8118-bf9666a225f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([   0, 1735, 4675, 8821,   25, 1706,  119, 1842, 7646,    4,  235, 2156,\n",
              "           94,  391,   17, 5015,    5,    2,    1,    1,    1,    1,    1,    1,\n",
              "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "            1,    1,    1,    1,    1,    1,    1,    1]),\n",
              " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " 'labels': tensor([-100,   12, -100, -100,   12,    8,    4,    4,    4,   12,   12,   12,\n",
              "           12,   12,   12,   12,   12, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100])}"
            ]
          },
          "execution_count": 136,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYj1EM_stz_i",
        "outputId": "f772d5da-4d34-4c2d-cf5e-844fe1d43f2c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at quocanh944/phoBERT-ner and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([9]) in the checkpoint and torch.Size([15]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([9, 768]) in the checkpoint and torch.Size([15, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForTokenClassification\n",
        "\n",
        "# Đảm bảo bạn đã có label_list\n",
        "num_labels = len(label_list)  # VD: O, B-HOTEL, I-HOTEL, B-RESTAURANT, ...\n",
        "\n",
        "# Load PhoBERT for token classification\n",
        "# model = AutoModelForTokenClassification.from_pretrained(\n",
        "#     \"vinai/phobert-base\",\n",
        "#     num_labels=num_labels\n",
        "# )\n",
        "\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    \"quocanh944/phoBERT-ner\",\n",
        "    num_labels=num_labels,\n",
        "    ignore_mismatched_sizes=True  # Cho phép load weight cũ không khớp head\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "PKUNeRdslrTw"
      },
      "outputs": [],
      "source": [
        "for param in model.base_model.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-jm640i1hVG",
        "outputId": "28c41f7e-62c0-4ed6-a7ab-801e6ff2abb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.52.4\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "print(transformers.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "jCyluF-1-QnS"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "yFIDAOsc-bPp"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',                     # Nơi lưu checkpoint\n",
        "    per_device_train_batch_size=16,             # Batch size train\n",
        "    per_device_eval_batch_size=16,              # Batch size eval\n",
        "    num_train_epochs=2,                         # Số epoch train\n",
        "    weight_decay=0.01,                          # Weight decay\n",
        "    logging_dir='./logs',                       # Thư mục log\n",
        "    logging_strategy=\"steps\",                   # Chiến lược logging\n",
        "    logging_steps=10,                           # Log mỗi 10 step\n",
        "    save_strategy=\"steps\",                      # Chiến lược save\n",
        "    save_steps=500,                             # Save mỗi 500 step\n",
        "    save_total_limit=2,                         # Tối đa 2 checkpoint\n",
        "    report_to=\"none\"                            # Không gửi log lên wandb\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDFFu4RTt1_p"
      },
      "outputs": [],
      "source": [
        "# from transformers import TrainingArguments\n",
        "\n",
        "# # Define training arguments\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir='./results',           # output directory to save model checkpoints and results\n",
        "#     evaluation_strategy=\"epoch\",      # evaluation is done at the end of every epoch\n",
        "#     per_device_train_batch_size=16,   # batch size per device during training\n",
        "#     per_device_eval_batch_size=16,    # batch size for evaluation\n",
        "#     num_train_epochs=3,               # number of epochs to train the model\n",
        "#     weight_decay=0.01,                # strength of weight decay\n",
        "#     logging_dir='./logs',             # directory for storing logs\n",
        "#     logging_steps=10,                 # log every 10 steps\n",
        "#     save_steps=500,                   # save model checkpoint every 500 steps\n",
        "#     save_total_limit=2,               # limit the number of total checkpoints to save\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "DtZ_OWOM0vS8",
        "outputId": "dbac93bc-2b8d-4632-85eb-d50eb4007c3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from seqeval) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=9fd50e7f435f918afd8916b64116215ccc83d436962ecbb646fe75ab91a256b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/92/f0/243288f899c2eacdfa8c5f9aede4c71a9bad0ee26a01dc5ead\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install seqeval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNBgsfBj0v2w",
        "outputId": "7ba41c43-f68b-468c-f916-44c5bbe7894c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['I-RENTALSERVICES',\n",
              " 'I-STREETFOODRESTAURANT',\n",
              " 'I-TOUR',\n",
              " 'B-HOTELS',\n",
              " 'I-RESTAURANTS',\n",
              " 'B-STREETFOODRESTAURANT',\n",
              " 'I-DRINKPLACES',\n",
              " 'I-HOTELS',\n",
              " 'B-RESTAURANTS',\n",
              " 'B-DRINKPLACES',\n",
              " 'I-ATTRACTIONS',\n",
              " 'B-ATTRACTIONS',\n",
              " 'O',\n",
              " 'B-RENTALSERVICES',\n",
              " 'B-TOUR']"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "label_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "_IZlAoeuILE4"
      },
      "outputs": [],
      "source": [
        "id2label = {i: label for i, label in enumerate(label_list)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "pB5aNXGj8BCS"
      },
      "outputs": [],
      "source": [
        "# !pip uninstall transformers\n",
        "# !pip install transformers==4.53.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "W_Z3Jzv03vBO"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
        "from transformers import EvalPrediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "NojEUal83YBP"
      },
      "outputs": [],
      "source": [
        "from transformers import EvalPrediction\n",
        "def compute_metrics(p: EvalPrediction):\n",
        "    predictions = p.predictions.argmax(axis=2)  # Get predicted label indices\n",
        "    labels = p.label_ids  # True label IDs\n",
        "\n",
        "    # Debugging: Print shapes of predictions and labels\n",
        "    print(f\"Shape of predictions: {predictions.shape}\")\n",
        "    print(f\"Shape of labels: {labels.shape}\")\n",
        "\n",
        "    # Debugging: Log first few predictions and labels for inspection\n",
        "    print(f\"First few predictions: {predictions[:2]}\")\n",
        "    print(f\"First few labels: {labels[:2]}\")\n",
        "\n",
        "    pred_labels = []\n",
        "    true_labels = []\n",
        "\n",
        "    # Iterate through predictions and labels\n",
        "    for i, (pred_seq, true_seq) in enumerate(zip(predictions, labels)):\n",
        "        pred_label_seq = []\n",
        "        true_label_seq = []\n",
        "\n",
        "        # Iterate through each token in the sequence\n",
        "        for pred_idx, true_idx in zip(pred_seq, true_seq):\n",
        "            if true_idx == -100:\n",
        "                # Debugging: Log any padding tokens encountered\n",
        "                # print(f\"Padding token encountered at position {i}\")\n",
        "                continue\n",
        "\n",
        "            # Check if the indices are within the valid range\n",
        "            if pred_idx < len(label_list) and true_idx < len(label_list):\n",
        "                pred_label_seq.append(label_list[pred_idx])\n",
        "                true_label_seq.append(label_list[true_idx])\n",
        "            else:\n",
        "                # Debugging: Log when out-of-bound indices are encountered\n",
        "                print(f\"Index out of range: pred_idx={pred_idx}, true_idx={true_idx} at position {i}\")\n",
        "\n",
        "        pred_labels.append(pred_label_seq)\n",
        "        true_labels.append(true_label_seq)\n",
        "\n",
        "    # Debugging: Log final processed predictions and labels\n",
        "    print(f\"Processed pred_labels: {pred_labels[:2]}\")\n",
        "    print(f\"Processed true_labels: {true_labels[:2]}\")\n",
        "\n",
        "    # Compute token-level F1, Precision, and Recall\n",
        "    precision = precision_score(true_labels, pred_labels)\n",
        "    # Trong 10 lần dự đoán nhãn PER: thì chúng ta đoán đúng 6 lần -> 6/10 = 60%\n",
        "\n",
        "    recall = recall_score(true_labels, pred_labels)\n",
        "    # Trong 8 nhãn PER thật: thì chúng ta đoán đúng 6 lần -> 6/8 = 75%\n",
        "\n",
        "    f1 = f1_score(true_labels, pred_labels)\n",
        "\n",
        "    # Debugging: Print classification report\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(true_labels, pred_labels))\n",
        "\n",
        "    return {\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "8YK9fFyA3bTW"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
        "from transformers import EvalPrediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "mjzY2g4c3dOx",
        "outputId": "0182eb78-ee7c-4fec-e68d-66995890b16a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-154-2719278411.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='126' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [126/126 16:54, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.164500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.071600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.994100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.922900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.850200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.816900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.762500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.725700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.697900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.681000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.664600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.657800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=126, training_loss=1.8241927888658311, metrics={'train_runtime': 1022.5711, 'train_samples_per_second': 1.956, 'train_steps_per_second': 0.123, 'total_flos': 130663733760000.0, 'train_loss': 1.8241927888658311, 'epoch': 2.0})"
            ]
          },
          "execution_count": 154,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize the Trainer with the modified compute_metrics function\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    # eval_dataset=dev_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics  # Updated function\n",
        "\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "OUhJncKE3e6c"
      },
      "outputs": [],
      "source": [
        "model = trainer.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "oOmxDlX6H7CM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def predict_ner(text, model, tokenizer, id2label):\n",
        "    # Tokenize\n",
        "    # text = text.lower()\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        is_split_into_words=False\n",
        "    )\n",
        "\n",
        "    # Predict\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "    # Decode tokens và labels\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "    print(tokens)\n",
        "    labels = [id2label[label_id.item()] for label_id in predictions[0]]\n",
        "\n",
        "    # Hiển thị sạch\n",
        "    for token, label in zip(tokens, labels):\n",
        "        token_clean = token.replace(\"▁\", \"\") if \"▁\" in token else token\n",
        "        print(f\"{token_clean}\\t{label}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UL_f1QiJIU-u",
        "outputId": "66415730-1d8d-440b-c81f-e3a7ba08d38b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['<s>', 'tôi', 'ăn', 'ở', 'Quán', 'Bánh', 'C@@', 'ă@@', 'n@@', ',', 'đi', 'chơi', 'ở', 'Hồ', 'Xuân', 'Hương', '</s>']\n",
            "<s>\tO\n",
            "tôi\tO\n",
            "ăn\tO\n",
            "ở\tO\n",
            "Quán\tO\n",
            "Bánh\tO\n",
            "C@@\tO\n",
            "ă@@\tO\n",
            "n@@\tO\n",
            ",\tO\n",
            "đi\tO\n",
            "chơi\tO\n",
            "ở\tO\n",
            "Hồ\tO\n",
            "Xuân\tO\n",
            "Hương\tO\n",
            "</s>\tO\n"
          ]
        }
      ],
      "source": [
        "text = \"tôi ăn ở Quán Bánh Căn, đi chơi ở Hồ Xuân Hương\"\n",
        "predict_ner(text, model, tokenizer, id2label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pb-hbaaXIZ8-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
