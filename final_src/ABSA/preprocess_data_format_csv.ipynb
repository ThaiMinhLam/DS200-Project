{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec5886b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cfb23e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'F:\\\\Studies\\\\Third_year\\\\Big_data\\\\Final_Code\\\\src1\\\\ML\\\\ABSA\\\\data\\\\train_campings.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m train_data = extract_info_csv(raw_train)\n\u001b[32m     15\u001b[39m test_data = extract_info_csv(raw_test)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mF:\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mStudies\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mThird_year\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mBig_data\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mFinal_Code\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43msrc1\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mML\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mABSA\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mtrain_campings.json\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mw\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     18\u001b[39m     json.dump(train_data, f, ensure_ascii=\u001b[38;5;28;01mFalse\u001b[39;00m, indent=\u001b[32m2\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33mF:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mStudies\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mThird_year\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mBig_data\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mFinal_Code\u001b[39m\u001b[33m\\\u001b[39m\u001b[33msrc1\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mML\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mABSA\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mtest_campings.json\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADMIN\\.conda\\envs\\prj__env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:326\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    321\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    324\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'F:\\\\Studies\\\\Third_year\\\\Big_data\\\\Final_Code\\\\src1\\\\ML\\\\ABSA\\\\data\\\\train_campings.json'"
     ]
    }
   ],
   "source": [
    "def extract_info_csv(df):\n",
    "    data = []\n",
    "    for i in range(len(df)):\n",
    "        label_dict = ast.literal_eval(df.loc[i,'aspects'])\n",
    "        if len(label_dict) != 0:\n",
    "            text = df.loc[i,'text']\n",
    "            data.append({str(i): {'text': text, 'label': label_dict}})\n",
    "            \n",
    "    return data\n",
    "\n",
    "raw_train = pd.read_csv(r\"F:\\Studies\\Third_year\\Big_data\\Final_Code\\Data\\ABSA_Dataset\\final_campings_ABSA_train.csv\", index_col=0)\n",
    "raw_test = pd.read_csv(r\"F:\\Studies\\Third_year\\Big_data\\Final_Code\\Data\\ABSA_Dataset\\final_campings_ABSA_test.csv\", index_col=0)\n",
    "\n",
    "train_data = extract_info_csv(raw_train)\n",
    "test_data = extract_info_csv(raw_test)\n",
    "\n",
    "with open(r'F:\\Studies\\Third_year\\Big_data\\Final_Code\\src1\\ML\\ABSA\\data\\train_campings.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(train_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(r'F:\\Studies\\Third_year\\Big_data\\Final_Code\\src1\\ML\\ABSA\\data\\test_campings.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(test_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ Split xong: {len(train_data)} train, {len(test_data)} test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eeb725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw = pd.read_csv(r\"F:\\Studies\\Third_year\\Big_data\\Final_Code\\Data\\ABSA_Dataset\\final_drinkplaces_ABSA_train.csv\", index_col=0)\n",
    "\n",
    "# data = []\n",
    "# for i in range(len(raw)):\n",
    "#     label_dict = ast.literal_eval(raw.loc[i,'aspects'])\n",
    "#     if len(label_dict) != 0:\n",
    "#         text = raw.loc[i,'text']\n",
    "#         data.append({str(i): {'text': text, 'label': label_dict}})\n",
    "    \n",
    "# random.shuffle(data)\n",
    "\n",
    "# n = len(data)\n",
    "# train_end = int(0.8 * n)\n",
    "# val_end = int(0.9 * n)\n",
    "\n",
    "# train_data = data[:train_end]\n",
    "# val_data = data[train_end:val_end]\n",
    "# test_data = data[val_end:]\n",
    "\n",
    "# with open(r'F:\\Studies\\Third_year\\Big_data\\Final_Code\\src\\ThamKhao\\ABSA-VLSP2018\\data\\train_cafe.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(train_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# with open(r'F:\\Studies\\Third_year\\Big_data\\Final_Code\\src\\ThamKhao\\ABSA-VLSP2018\\data\\val_cafe.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(val_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# with open(r'F:\\Studies\\Third_year\\Big_data\\Final_Code\\src\\ThamKhao\\ABSA-VLSP2018\\data\\test_cafe.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(test_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# print(f\"✅ Split xong: {len(train_data)} train, {len(val_data)} val, {len(test_data)} test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c37490d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Split xong: 42472 train, 5309 val, 5309 test\n"
     ]
    }
   ],
   "source": [
    "# raw = pd.read_csv(r\"F:\\Studies\\Third_year\\Big_data\\Final_Code\\src\\ThamKhao\\ABSA-VLSP2018\\data\\list_aspects_drinkplaces.csv\", index_col=0)\n",
    "\n",
    "# data = []\n",
    "# for i in range(len(raw)):\n",
    "#     label_dict = ast.literal_eval(raw.loc[i,'aspects'])\n",
    "#     if len(label_dict) != 0:\n",
    "#         text = raw.loc[i,'text']\n",
    "#         data.append({str(i): {'text': text, 'label': label_dict}})\n",
    "    \n",
    "# random.shuffle(data)\n",
    "\n",
    "# n = len(data)\n",
    "# train_end = int(0.8 * n)\n",
    "# val_end = int(0.9 * n)\n",
    "\n",
    "# train_data = data[:train_end]\n",
    "# val_data = data[train_end:val_end]\n",
    "# test_data = data[val_end:]\n",
    "\n",
    "# with open(r'F:\\Studies\\Third_year\\Big_data\\Final_Code\\src\\ThamKhao\\ABSA-VLSP2018\\data\\train_cafe.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(train_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# with open(r'F:\\Studies\\Third_year\\Big_data\\Final_Code\\src\\ThamKhao\\ABSA-VLSP2018\\data\\val_cafe.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(val_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# with open(r'F:\\Studies\\Third_year\\Big_data\\Final_Code\\src\\ThamKhao\\ABSA-VLSP2018\\data\\test_cafe.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(test_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# print(f\"✅ Split xong: {len(train_data)} train, {len(val_data)} val, {len(test_data)} test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa024df5",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "sentiment_map = {'NEGATIVE': 'negative', 'NEUTRAL': 'neutral', 'POSITIVE': 'positive'}\n",
    "\n",
    "def json_to_txt(json_file, txt_file):\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    with open(txt_file, 'w', encoding='utf-8') as f:\n",
    "        for idx, item in enumerate(data, start=1):\n",
    "            inner = list(item.values())[0]\n",
    "            text = inner['text'].replace('\\n', ' ').replace('\\r', ' ').strip()\n",
    "            label_dict = inner['label']\n",
    "\n",
    "            f.write(f'#{idx}\\n')\n",
    "            f.write(f'{text}\\n')\n",
    "\n",
    "            label_list = [f'{{{aspect}, {sentiment_map[sentiment]}}}' \n",
    "                          for aspect, sentiment in label_dict.items()]\n",
    "            label_line = ', '.join(label_list)\n",
    "            f.write(f'{label_line}\\n\\n')\n",
    "\n",
    "\n",
    "json_to_txt(r'F:\\Studies\\Third_year\\Big_data\\Final_Code\\src\\ML\\ABSA-VLSP2018\\data\\train_attractions.json', r'F:\\Studies\\Third_year\\Big_data\\Final_Code\\src\\ML\\ABSA-VLSP2018\\data\\train_attractions.txt')\n",
    "# json_to_txt(r'F:\\Studies\\Third_year\\Big_data\\Final_Code\\src\\ThamKhao\\ABSA-VLSP2018\\data\\val_cafe.json', r'F:\\Studies\\Third_year\\Big_data\\Final_Code\\src\\ThamKhao\\ABSA-VLSP2018\\data\\val_cafe.txt')\n",
    "json_to_txt(r'F:\\Studies\\Third_year\\Big_data\\Final_Code\\src\\ML\\ABSA-VLSP2018\\data\\test_attractions.json', r'F:\\Studies\\Third_year\\Big_data\\Final_Code\\src\\ML\\ABSA-VLSP2018\\data\\test_attractions.txt')\n",
    "\n",
    "# with open(r\"F:\\Studies\\Third_year\\Big_data\\Final_Code\\src\\ThamKhao\\ABSA-VLSP2018\\data\\cafe_review.txt\",'w', encoding='utf-8') as f:\n",
    "#     for idx, item in enumerate(data, start=1):\n",
    "#         inner = list(item.values())[0]\n",
    "#         text = inner['text'].replace('\\n', ' ').strip()\n",
    "\n",
    "#         label_dict = inner['label']\n",
    "\n",
    "#         f.write(f'#{idx}\\n')\n",
    "\n",
    "#         f.write(f'{text}\\n')\n",
    "        \n",
    "#         label_list = [f'{{{aspect}, {sentiment_map[sentiment]}}}' for aspect, sentiment in label_dict.items()]\n",
    "#         label_line = ', '.join(label_list)\n",
    "#         f.write(f'{label_line}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbd3da6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prj__env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
