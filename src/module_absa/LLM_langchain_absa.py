import os
import google.generativeai as genai
import json
import pandas as pd
import re
import requests
from io import StringIO
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).resolve().parents[2]))
from src.config import Config
os.environ["GOOGLE_API_KEY"] = Config.GOOGLE_API_KEY2
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain_google_genai import ChatGoogleGenerativeAI

ASPECTS_DICT = {
    "HOTELS": [
        "HOTEL#LOCATION", "HOTEL#QUALITY", "HOTEL#FACILITIES", "HOTEL#STYLE",
        "WIFI", "PRICE", "ROOM#QUALITY", "ROOM#STYLE", "ROOM#FACILITIES",
        "ROOM#SOUND", "ROOM#VIEW", "ROOM#ATMOSPHERE", "ROOM#CLEANLINESS",
        "SERVICE#STAFF", "SERVICE#CHECKIN"
    ],
    "RESTAURANTS": [
        "LOCATION", "PRICE", "FOOD#QUALITY", "FOOD#VARIETY",
        "FOOD#PRESENTATION", "FOOD#FRESHNESS", "DRINK#QUALITY",
        "ENVIRONMENT#CLEANLINESS", "ENVIRONMENT#AMBIENCE",
        "SERVICE#STAFF", "SERVICE#ORDER"
    ],
    "DRINKPLACES": [
        "LOCATION", "PRICE", "FOOD#QUALITY", "DRINK#QUALITY",
        "DRINK#VARIETY", "ENVIRONMENT#CLEANLINESS", "ENVIRONMENT#AMBIENCE",
        "SERVICE#STAFF", "SERVICE#ORDER"
    ],
    "STREETFOODRESTAURANT": [
        "LOCATION", "PRICE", "FOOD#QUALITY", "FOOD#VARIETY",
        "DRINK#QUALITY", "DRINK#VARIETY", "ENVIRONMENT#CLEANLINESS",
        "ENVIRONMENT#AMBIENCE", "SERVICE#STAFF", "SERVICE#ORDER"
    ],
    "ATTRACTIONS": [
        "LOCATION", "PRICE", "SERVICE#STAFF", "SERVICE#BOOKING",
        "ENVIRONMENT#SCENERY", "ENVIRONMENT#ATMOSPHERE",
        "EXPERIENCE#ACTIVITY"
    ],
    "RENTALSERVICES": [
        "LOCATION", "PRICE", "SERVICE#RENTING", "SERVICE#STAFF",
        "VEHICLE#QUALITY"
    ],
    "TOURS": [
        "LOCATION", "PRICE", "SERVICE#STAFF", "EXPERIENCE#ACTIVITY",
        "ENVIRONMENT#SCENERY", "ENVIRONMENT#ATMOSPHERE"
    ],
    "CAMPINGS": [
        "LOCATION#DISTANCE", "LOCATION#ACCESSIBILITY", "SERVICE#STAFF",
        "ENVIRONMENT#SCENERY", "ENVIRONMENT#WEATHER", "ENVIRONMENT#ATMOSPHERE"
    ]
}

class VietnameseNormalizer:
    """
    Tham kh·∫£o: https://github.com/VinAIResearch/BARTpho/blob/main/VietnameseToneNormalization.md
    """
    VINAI_NORMALIZED_TONE = {
        '√≤a': 'o√†', '√ía': 'O√†', '√íA': 'O√Ä', 
        '√≥a': 'o√°', '√ìa': 'O√°', '√ìA': 'O√Å', 
        '·ªèa': 'o·∫£', '·ªéa': 'O·∫£', '·ªéA': 'O·∫¢',
        '√µa': 'o√£', '√ïa': 'O√£', '√ïA': 'O√É',
        '·ªça': 'o·∫°', '·ªåa': 'O·∫°', '·ªåA': 'O·∫†',
        '√≤e': 'o√®', '√íe': 'O√®', '√íE': 'O√à',
        '√≥e': 'o√©', '√ìe': 'O√©', '√ìE': 'O√â',
        '·ªèe': 'o·∫ª', '·ªée': 'O·∫ª', '·ªéE': 'O·∫∫',
        '√µe': 'o·∫Ω', '√ïe': 'O·∫Ω', '√ïE': 'O·∫º',
        '·ªçe': 'o·∫π', '·ªåe': 'O·∫π', '·ªåE': 'O·∫∏',
        '√πy': 'u·ª≥', '√ôy': 'U·ª≥', '√ôY': 'U·ª≤',
        '√∫y': 'u√Ω', '√öy': 'U√Ω', '√öY': 'U√ù',
        '·ªßy': 'u·ª∑', '·ª¶y': 'U·ª∑', '·ª¶Y': 'U·ª∂',
        '≈©y': 'u·ªπ', '≈®y': 'U·ªπ', '≈®Y': 'U·ª∏',
        '·ª•y': 'u·ªµ', '·ª§y': 'U·ªµ', '·ª§Y': 'U·ª¥',
    }
    
    @staticmethod
    def normalize_unicode(text):
        char1252 = r'aÃÄ|aÃÅ|aÃâ|aÃÉ|aÃ£|√¢ÃÄ|√¢ÃÅ|√¢Ãâ|√¢ÃÉ|√¢Ã£|ƒÉÃÄ|ƒÉÃÅ|ƒÉÃâ|ƒÉÃÉ|ƒÉÃ£|eÃÄ|eÃÅ|eÃâ|eÃÉ|eÃ£|√™ÃÄ|√™ÃÅ|√™Ãâ|√™ÃÉ|√™Ã£|iÃÄ|iÃÅ|iÃâ|iÃÉ|iÃ£|oÃÄ|oÃÅ|oÃâ|oÃÉ|oÃ£|√¥ÃÄ|√¥ÃÅ|√¥Ãâ|√¥ÃÉ|√¥Ã£|∆°ÃÄ|∆°ÃÅ|∆°Ãâ|∆°ÃÉ|∆°Ã£|uÃÄ|uÃÅ|uÃâ|uÃÉ|uÃ£|∆∞ÃÄ|∆∞ÃÅ|∆∞Ãâ|∆∞ÃÉ|∆∞Ã£|yÃÄ|yÃÅ|yÃâ|yÃÉ|yÃ£|AÃÄ|AÃÅ|AÃâ|AÃÉ|AÃ£|√ÇÃÄ|√ÇÃÅ|√ÇÃâ|√ÇÃÉ|√ÇÃ£|ƒÇÃÄ|ƒÇÃÅ|ƒÇÃâ|ƒÇÃÉ|ƒÇÃ£|EÃÄ|EÃÅ|EÃâ|EÃÉ|EÃ£|√äÃÄ|√äÃÅ|√äÃâ|√äÃÉ|√äÃ£|IÃÄ|IÃÅ|IÃâ|IÃÉ|IÃ£|OÃÄ|OÃÅ|OÃâ|OÃÉ|OÃ£|√îÃÄ|√îÃÅ|√îÃâ|√îÃÉ|√îÃ£|∆†ÃÄ|∆†ÃÅ|∆†Ãâ|∆†ÃÉ|∆†Ã£|UÃÄ|UÃÅ|UÃâ|UÃÉ|UÃ£|∆ØÃÄ|∆ØÃÅ|∆ØÃâ|∆ØÃÉ|∆ØÃ£|YÃÄ|YÃÅ|YÃâ|YÃÉ|YÃ£'
        charutf8 = r'√†|√°|·∫£|√£|·∫°|·∫ß|·∫•|·∫©|·∫´|·∫≠|·∫±|·∫Ø|·∫≥|·∫µ|·∫∑|√®|√©|·∫ª|·∫Ω|·∫π|·ªÅ|·∫ø|·ªÉ|·ªÖ|·ªá|√¨|√≠|·ªâ|ƒ©|·ªã|√≤|√≥|·ªè|√µ|·ªç|·ªì|·ªë|·ªï|·ªó|·ªô|·ªù|·ªõ|·ªü|·ª°|·ª£|√π|√∫|·ªß|≈©|·ª•|·ª´|·ª©|·ª≠|·ªØ|·ª±|·ª≥|√Ω|·ª∑|·ªπ|·ªµ|√Ä|√Å|·∫¢|√É|·∫†|·∫¶|·∫§|·∫®|·∫™|·∫¨|·∫∞|·∫Æ|·∫≤|·∫¥|·∫∂|√à|√â|·∫∫|·∫º|·∫∏|·ªÄ|·∫æ|·ªÇ|·ªÑ|·ªÜ|√å|√ç|·ªà|ƒ®|·ªä|√í|√ì|·ªé|√ï|·ªå|·ªí|·ªê|·ªî|·ªñ|·ªò|·ªú|·ªö|·ªû|·ª†|·ª¢|√ô|√ö|·ª¶|≈®|·ª§|·ª™|·ª®|·ª¨|·ªÆ|·ª∞|·ª≤|√ù|·ª∂|·ª∏|·ª¥'
        char_map = dict(zip(char1252.split('|'), charutf8.split('|')))
        return re.sub(char1252, lambda x: char_map[x.group()], text.strip())

    @staticmethod
    def normalize_typing(text):
        for wrong_word, correct_word in VietnameseNormalizer.VINAI_NORMALIZED_TONE.items():
            text = text.replace(wrong_word, correct_word)
        return text.strip()

class VietnameseCleaner:
    def remove_emoji(text):
        emoji_pattern = re.compile("["
            u"\U0001F600-\U0001F64F"  # emoticons
            u"\U0001F300-\U0001F5FF"  # symbols & pictographs
            u"\U0001F680-\U0001F6FF"  # transport & map symbols
            u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
            u"\U00002500-\U00002BEF"  # chinese char
            u"\U00002702-\U000027B0"
            u"\U000024C2-\U0001F251"
            u"\U0001f926-\U0001f937"
            u"\U00010000-\U0010ffff"
            u"\U0001F600-\U0001F64F"
            u"\U0001F300-\U0001F5FF"
            u"\U0001F680-\U0001F6FF"
            u"\U0001F1E0-\U0001F1FF"
            u"\U00002700-\U000027BF"
            u"\U000024C2-\U0001F251"            
                            "]+", flags=re.UNICODE)
        return emoji_pattern.sub(r'', text)

    def remove_punctuation_emoji(text):
        # X√≥a c√°c emoji: :))), :(((, =)))), =(((, ...
        text = re.sub(r'[:;=xX8@‚Ç´&]+-?[)(DPpOo3v]+', '', text)
        text = re.sub(r'[)(DPpOo3v]+-?[:;=xX8@‚Ç´&]+', '', text)
        text = re.sub(r'[:;=xX8@‚Ç´&]+[)(DPpOo3v]+', '', text)
        
        # X√≥a c√°c emoji @@, @.@, =.=, ...
        text = re.sub(r'[@=^~*]([.o_-])?[@=^~*]', '', text)
        
        # X√≥a ngo·∫∑c th·ª´a
        text = re.sub(r'\(\)', '', text)
        return text.strip()

    def remove_uncharacter_Vietnamese(text):
        ALLOWED_PUNCTUATION = r'\.,!?‚Äì:;'
        VN_CHARS = '√°√†·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√©√®·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√≥√≤·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√≠√¨·ªâƒ©·ªã√∫√π·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±√Ω·ª≥·ª∑·ªπ·ªµƒë√Å√Ä·∫¢√É·∫†ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨√â√à·∫∫·∫º·∫∏√ä·∫æ·ªÄ·ªÇ·ªÑ·ªÜ√ì√í·ªé√ï·ªå√î·ªê·ªí·ªî·ªñ·ªò∆†·ªö·ªú·ªû·ª†·ª¢√ç√å·ªàƒ®·ªä√ö√ô·ª¶≈®·ª§∆Ø·ª®·ª™·ª¨·ªÆ·ª∞√ù·ª≤·ª∂·ª∏·ª¥ƒê'
        text = re.sub(fr'[^\sa-zA-Z0-9{VN_CHARS}{ALLOWED_PUNCTUATION}]', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        return text

    @staticmethod
    def clean_social_text(text):
        text = VietnameseCleaner.remove_emoji(text)
        text = VietnameseCleaner.remove_punctuation_emoji(text)
        
        # remove html
        text = re.sub(r'<[^>]*>', '', text)
        
        # remove hashtag
        text = re.sub(r'#\w+', '', text)
        
        # remove url
        text = re.sub(r'https?://\S+|www\.\S+', '', text)
        
        # remove hotline
        text = re.sub(r'\b[(]?(\+84|0)[)]?\d{3}[-\s\.]?\d{3}[-\s\.]?\d{3,6}\b', '', text)
        
        # remove email
        text = re.sub(r'[^@ \t\r\n]+@[^@ \t\r\n]+\.[^@ \t\r\n]+', '', text)
        
        # remove repeated characters (gi·∫£m b·ªõt c∆∞·ªùng ƒë·ªô c·ªßa t·ª´)
        text = re.sub(r'(.)\1{2,}', r'\1\1', text)

        # remove uncharacter + extra whitespace
        text = VietnameseCleaner.remove_uncharacter_Vietnamese(text)
        return text


class VietnameseTextProcessor:
    def __init__(self, max_correction_length=512):
        self.max_correction_length = max_correction_length
        self._build_teencodes()
    
    def _build_teencodes(self):
        self.teencodes = {
            'ok': ['okie', 'okey', '√¥k√™', 'oki', 'oke', 'okay', 'ok√™'], 
            'kh√¥ng': ['kg', 'not', 'k', 'kh', 'k√¥', 'hok', 'ko', 'khong'], 'kh√¥ng ph·∫£i': ['kp'], 
            'c·∫£m ∆°n': ['tks', 'thks', 'thanks', 'ths', 'thank'], 'h·ªìi ƒë√≥': ['h√πi ƒë√≥'], 'mu·ªën': ['m√∫n'],
            
            'r·∫•t t·ªët': ['perfect', '‚ù§Ô∏è', 'üòç'], 'd·ªÖ th∆∞∆°ng': ['cute'], 'y√™u': ['iu'], 'th√≠ch': ['thik'], 
            't·ªët': [
                'gud', 'good', 'g√∫t', 'tot', 'nice',
                'hehe', 'hihi', 'haha', 'hjhj', 'thick', '^_^', ':)', '=)'
                'üëç', 'üéâ', 'üòÄ', 'üòÇ', 'ü§ó', 'üòô', 'üôÇ'
            ], 
            'b√¨nh th∆∞·ªùng': ['bt', 'bthg'], 'h√†g': ['h√†ng'], 
            'kh√¥ng t·ªët':  ['lol', 'cc', 'huhu', ':(', 'üòî', 'üòì'],
            't·ªá': ['sad', 'por', 'poor', 'bad'], 'gi·∫£ m·∫°o': ['fake'], 
            
            'qu√°': ['wa', 'w√°', 'q√°'], 'ƒë∆∞·ª£c': ['ƒëx', 'dk', 'dc', 'ƒëk', 'ƒëc'], 
            'v·ªõi': ['vs'], 'g√¨': ['j'], 'r·ªìi': ['r'], 'm√¨nh': ['m', 'mik'], 
            'th·ªùi gian': ['time'], 'gi·ªù': ['h'], 
        }
                
        self.teencodes = {word: key for key, values in self.teencodes.items() for word in values}
        teencode_url = 'https://gist.githubusercontent.com/behitek/7d9441c10b3c2739499fc5a4d9ea06fb/raw/df939245b3e841b62af115be4dcb3516dadc9fc5/teencode.txt'
        response = requests.get(teencode_url)
        
        if response.status_code == 200:
            text_data = StringIO(response.text)
            for pair in text_data:
                teencode, true_text = pair.split('\t')
                self.teencodes[teencode.strip()] = true_text.strip()
            self.teencodes = {k: self.teencodes[k] for k in sorted(self.teencodes)}
        else: print('Failed to fetch teencode.txt from', teencode_url)
        
    def correct_vietnamese_errors(self, texts):
        # https://huggingface.co/bmd1905/vietnamese-correction
        predictions = self.corrector(texts, max_length=self.max_correction_length, truncation=True)
        return [prediction['generated_text'] for prediction in predictions]
    
    def normalize_teencodes(self, text):
        words = []
        for word in text.split():
            words.append(self.teencodes.get(word, word))
        return ' '.join(words)
    
    def process_text(self, text, normalize_tone=True):
        # text = text.lower()
        if normalize_tone:
            text = VietnameseNormalizer.normalize_unicode(text)
            text = VietnameseNormalizer.normalize_typing(text)
        text = VietnameseCleaner.clean_social_text(text)
        text = self.normalize_teencodes(text)
        return text
    
class ABSAExtractor:
    def __init__(self, llm):
        self.prompt_template = PromptTemplate(
            input_variables=["text", "domain", "aspects"],
            template="""
B·∫°n l√† c√¥ng c·ª• Aspect-Based Sentiment Analysis (ABSA) cho **review ti·∫øng Vi·ªát** v·ªÅ d·ªãch v·ª• trong lƒ©nh v·ª±c **{domain}** t·∫°i ƒê√† L·∫°t, Vi·ªát Nam.
Nhi·ªám v·ª•:
- X√°c ƒë·ªãnh c√°c aspect xu·∫•t hi·ªán trong review d∆∞·ªõi ƒë√¢y v√† g√°n sentiment ph√π h·ª£p.
- Ch·ªâ s·ª≠ d·ª•ng **danh s√°ch aspect d∆∞·ªõi ƒë√¢y**, kh√¥ng t·ª± nghƒ© th√™m aspect kh√°c.
- Sentiment ch·ªâ ƒë∆∞·ª£c g√°n: "POSITIVE", "NEUTRAL", "NEGATIVE".

### Danh s√°ch aspect:
{aspects}

### Quy ƒë·ªãnh output:
- Ch·ªâ tr·∫£ v·ªÅ **d·∫°ng list JSON**, m·ªói ph·∫ßn t·ª≠ g·ªìm:
{{"aspect": "ASPECT_NAME", "sentiment": "POSITIVE/NEUTRAL/NEGATIVE"}}
- N·∫øu kh√¥ng c√≥ aspect n√†o th√¨ tr·∫£ v·ªÅ: []

### Review:
{text}
"""
        )
        self.chain = LLMChain(llm=llm, prompt=self.prompt_template)

    def extract(self, text: str, domain: str, aspects_list: list, parse_json: bool = True):
        """
        text: c√¢u review c·∫ßn g√°n ABSA
        domain: t√™n domain (v√≠ d·ª•: HOTELS)
        aspects_list: list c√°c aspect (s·∫Ω convert sang d·∫°ng string ph√π h·ª£p)
        parse_json: n·∫øu True, t·ª± parse JSON tr·∫£ v·ªÅ, n·∫øu l·ªói th√¨ tr·∫£ raw string
        """
        aspects_formatted = "\n".join(f"- {aspect}" for aspect in aspects_list)

        result = self.chain.run(
            text=text,
            domain=domain,
            aspects=aspects_formatted
        )
        return result

    
@staticmethod
def clean_result(result):
    result = result.strip()
    if result.startswith("```"):
        # Lo·∫°i b·ªè d√≤ng ƒë·∫ßu ```json ho·∫∑c ```
        lines = result.split("\n")
        if lines[0].startswith("```"):
            lines = lines[1:]
        # Lo·∫°i b·ªè d√≤ng cu·ªëi ```
        if lines[-1].startswith("```"):
            lines = lines[:-1]
        result = "\n".join(lines).strip()
    return result

def absa_extract(text, domain, aspects_list):
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest", temperature=0)
    absa_extractor = ABSAExtractor(llm)
    result = absa_extractor.extract(text=text, domain=domain, aspects_list=aspects_list)
    cleaned_result = clean_result(result)
    return(json.loads(cleaned_result))
    
if __name__ == "__main__":
    text = "Ph√≤ng s·∫°ch, view ƒë·∫πp, nh√¢n vi√™n vui v·∫ª, gi√° h∆°i cao."
    domain = "HOTELS"
    aspects_list = ASPECTS_DICT[domain]
    result = absa_extract(text, domain, aspects_list)
    print(result)
